{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1237f52-e169-416e-b95b-1256516f0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc41062d-4df4-4b1c-b478-6dfba89bd358",
   "metadata": {},
   "source": [
    "## Set the device to CUDA (GPU) if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e64d11-8d3b-496c-be15-9684d890891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5450e4a-dd1f-4655-a717-815faac1901e",
   "metadata": {},
   "source": [
    "## Parameters used for data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5a38e2-858a-41e1-946b-aee4ffe0a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for generating array tokens\n",
    "MAX_INT_VALUE = 10  # Maximum value for integer elements\n",
    "MAX_FLOAT_VALUE = 1.0  # Maximum value for float elements\n",
    "DATASET_SIZE = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9051ce-f308-498a-b9a0-a49a6bb6d61f",
   "metadata": {},
   "source": [
    "## Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7df23c9-e16c-42a0-b3d3-59f0ea3aad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensionality of the token embeddings. This is the size of the vector\n",
    "# that will represent each token.\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "# The number of attention heads in the multi-head attention mechanism.\n",
    "# The embedding dimension must be divisible by this number.\n",
    "NUM_HEADS = 8\n",
    "\n",
    "# The number of Transformer encoder layers to stack.\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "\n",
    "# The dimension of the feed-forward network within each Transformer layer.\n",
    "FF_DIM = 1024\n",
    "\n",
    "# The dropout probability to be applied in the model.\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8cb3e-599f-40ff-acb2-26b29e366987",
   "metadata": {},
   "source": [
    "## Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5130a560-733a-43c1-ba5d-45633d4a96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of independent sequences to process in parallel.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# The length of the subsequences to be used for training. This is also known\n",
    "# as the context window or \"backpropagation through time\" (BPTT) length.\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "# The number of epochs to train the model for.\n",
    "EPOCHS = 5\n",
    "\n",
    "# The learning rate for the optimizer.\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# How often to log training progress (in batches).\n",
    "LOG_INTERVAL = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f974ae-6012-487e-b4ab-4e4245a5b7ce",
   "metadata": {},
   "source": [
    "## Generate random sequence for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d12f10-144a-48d0-bc06-4cd3b6193956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_array_token():\n",
    "    \"\"\"Generates a single array token [int, float, float, int].\"\"\"\n",
    "    token = [\n",
    "        np.random.randint(0, MAX_INT_VALUE),  # First int\n",
    "        round(np.random.uniform(0, MAX_FLOAT_VALUE), 2),  # First float (2 decimal places)\n",
    "        round(np.random.uniform(0, MAX_FLOAT_VALUE), 2),  # Second float (2 decimal places)\n",
    "        np.random.randint(0, MAX_INT_VALUE)   # Second int\n",
    "    ]\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a0870f2-9890-407b-8aee-1ce28925409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sequence(length, max_token_value=None):\n",
    "    \"\"\"Generates a long sequence of random array tokens.\"\"\"\n",
    "    tokens = []\n",
    "    for _ in range(length):\n",
    "        # Use a power law distribution for variety\n",
    "        if np.random.random() < 0.7:  # 70% chance for \"common\" patterns\n",
    "            # Common patterns might have smaller values\n",
    "            token = [\n",
    "                np.random.randint(0, MAX_INT_VALUE // 4),\n",
    "                round(np.random.uniform(0, MAX_FLOAT_VALUE / 4), 2),\n",
    "                round(np.random.uniform(0, MAX_FLOAT_VALUE / 4), 2),\n",
    "                np.random.randint(0, MAX_INT_VALUE // 4)\n",
    "            ]\n",
    "        elif np.random.random() < 0.9:  # 20% chance for medium patterns\n",
    "            token = [\n",
    "                np.random.randint(MAX_INT_VALUE // 4, MAX_INT_VALUE // 2),\n",
    "                round(np.random.uniform(MAX_FLOAT_VALUE / 4, MAX_FLOAT_VALUE / 2), 2),\n",
    "                round(np.random.uniform(MAX_FLOAT_VALUE / 4, MAX_FLOAT_VALUE / 2), 2),\n",
    "                np.random.randint(MAX_INT_VALUE // 4, MAX_INT_VALUE // 2)\n",
    "            ]\n",
    "        else:  # 10% chance for rare patterns\n",
    "            token = [\n",
    "                np.random.randint(MAX_INT_VALUE // 2, MAX_INT_VALUE),\n",
    "                round(np.random.uniform(MAX_FLOAT_VALUE / 2, MAX_FLOAT_VALUE), 2),\n",
    "                round(np.random.uniform(MAX_FLOAT_VALUE / 2, MAX_FLOAT_VALUE), 2),\n",
    "                np.random.randint(MAX_INT_VALUE // 2, MAX_INT_VALUE)\n",
    "            ]\n",
    "        tokens.append(token)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb2461-c325-49a3-8ea3-7182b8247b29",
   "metadata": {},
   "source": [
    "## Analyze Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4823c2af-38f2-4e4e-ad15-8a472daece1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_strings(tokens):\n",
    "    \"\"\"Convert array tokens to string representations for comparison.\"\"\"\n",
    "    return [str(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f544f11-a4bd-4746-b943-291fadb299a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_vocabulary(data_tokens):\n",
    "    \"\"\"\n",
    "    Analyzes the dataset to determine vocabulary characteristics.\n",
    "    Returns vocabulary size and other statistics.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing vocabulary from the dataset...\")\n",
    "    \n",
    "    # Convert tokens to strings for unique identification\n",
    "    token_strings = tokens_to_strings(data_tokens)\n",
    "    \n",
    "    # Count unique tokens\n",
    "    unique_strings = list(set(token_strings))\n",
    "    vocab_size = len(unique_strings)\n",
    "    \n",
    "    # Count frequencies\n",
    "    token_counts = {}\n",
    "    for token_str in token_strings:\n",
    "        token_counts[token_str] = token_counts.get(token_str, 0) + 1\n",
    "    \n",
    "    total_tokens = len(data_tokens)\n",
    "    \n",
    "    # Sort by frequency for analysis\n",
    "    sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Vocabulary Analysis Results:\")\n",
    "    print(f\"  - Total unique tokens (vocab size): {vocab_size}\")\n",
    "    print(f\"  - Total tokens in dataset: {total_tokens}\")\n",
    "    print(f\"  - Average token frequency: {total_tokens / vocab_size:.2f}\")\n",
    "    \n",
    "    # Show top 10 most frequent tokens\n",
    "    print(f\"  - Top 10 most frequent tokens:\")\n",
    "    for i in range(min(10, len(sorted_tokens))):\n",
    "        token_str, count = sorted_tokens[i]\n",
    "        percentage = (count / total_tokens) * 100\n",
    "        print(f\"    {token_str}: {count} occurrences ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Show some rare tokens\n",
    "    print(f\"  - Some rare tokens (bottom 5):\")\n",
    "    for i in range(max(0, len(sorted_tokens) - 5), len(sorted_tokens)):\n",
    "        token_str, count = sorted_tokens[i]\n",
    "        percentage = (count / total_tokens) * 100\n",
    "        print(f\"    {token_str}: {count} occurrences ({percentage:.2f}%)\")\n",
    "    \n",
    "    return vocab_size, {\n",
    "        'unique_strings': unique_strings,\n",
    "        'token_counts': token_counts,\n",
    "        'total_tokens': total_tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3cbcb5-f4e3-4bb8-aec0-b8ecf6b7f2b2",
   "metadata": {},
   "source": [
    "## Create token mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875ab8b8-8e37-4b0e-9c4c-8409da187d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_mapping(unique_strings, original_tokens):\n",
    "    \"\"\"\n",
    "    Creates a mapping from original array tokens to contiguous indices.\n",
    "    \"\"\"\n",
    "    print(\"Creating token mapping for efficient embedding...\")\n",
    "    \n",
    "    # Create string to original token mapping\n",
    "    str_to_token = {}\n",
    "    for token in original_tokens:\n",
    "        token_str = str(token)\n",
    "        if token_str not in str_to_token:\n",
    "            str_to_token[token_str] = token\n",
    "    \n",
    "    # Create mapping from token string to index\n",
    "    token_to_idx = {token_str: idx for idx, token_str in enumerate(unique_strings)}\n",
    "    idx_to_token = {idx: str_to_token[token_str] for idx, token_str in enumerate(unique_strings)}\n",
    "    \n",
    "    print(f\"  - Mapped {len(unique_strings)} unique array tokens to indices 0-{len(unique_strings)-1}\")\n",
    "    \n",
    "    return token_to_idx, idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0a0e597-9159-4c2e-813d-cfbfa9a63e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_dataset(data_tokens, token_to_idx):\n",
    "    \"\"\"\n",
    "    Remaps the dataset to use contiguous indices instead of original token arrays.\n",
    "    \"\"\"\n",
    "    print(\"Remapping dataset to use contiguous indices...\")\n",
    "    \n",
    "    # Create a new tensor with remapped values\n",
    "    remapped_indices = []\n",
    "    for token in data_tokens:\n",
    "        token_str = str(token)\n",
    "        remapped_indices.append(token_to_idx[token_str])\n",
    "    \n",
    "    remapped_data = torch.tensor(remapped_indices, dtype=torch.long)\n",
    "    \n",
    "    print(f\"  - Remapped {len(data_tokens)} array tokens\")\n",
    "    print(f\"  - Remapped token range: {remapped_data.min().item()} to {remapped_data.max().item()}\")\n",
    "    \n",
    "    return remapped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d367d23-d970-4044-a6c8-cb1359fc298e",
   "metadata": {},
   "source": [
    "## Creating raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22d59dd3-4eb4-4249-b4fa-7ada3e3f7596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random dataset of 100000 array tokens...\n",
      "Sample of raw data: [[1, 0.03, 0.08, 0], [0, 0.18, 0.03, 1], [1, 0.02, 0.13, 1], [1, 0.14, 0.19, 0], [2, 0.38, 0.44, 3]]...\n",
      "Analyzing vocabulary from the dataset...\n",
      "Vocabulary Analysis Results:\n",
      "  - Total unique tokens (vocab size): 11618\n",
      "  - Total tokens in dataset: 100000\n",
      "  - Average token frequency: 8.61\n",
      "  - Top 10 most frequent tokens:\n",
      "    [0, 0.18, 0.2, 0]: 45 occurrences (0.04%)\n",
      "    [1, 0.08, 0.2, 1]: 45 occurrences (0.04%)\n",
      "    [0, 0.24, 0.03, 0]: 44 occurrences (0.04%)\n",
      "    [0, 0.13, 0.23, 1]: 44 occurrences (0.04%)\n",
      "    [1, 0.21, 0.14, 1]: 43 occurrences (0.04%)\n",
      "    [1, 0.15, 0.03, 0]: 43 occurrences (0.04%)\n",
      "    [1, 0.05, 0.13, 1]: 43 occurrences (0.04%)\n",
      "    [0, 0.11, 0.21, 0]: 43 occurrences (0.04%)\n",
      "    [1, 0.15, 0.12, 1]: 43 occurrences (0.04%)\n",
      "    [0, 0.2, 0.16, 1]: 42 occurrences (0.04%)\n",
      "  - Some rare tokens (bottom 5):\n",
      "    [7, 0.75, 0.96, 5]: 1 occurrences (0.00%)\n",
      "    [7, 0.9, 0.77, 7]: 1 occurrences (0.00%)\n",
      "    [7, 0.74, 0.8, 5]: 1 occurrences (0.00%)\n",
      "    [5, 0.71, 0.57, 7]: 1 occurrences (0.00%)\n",
      "    [5, 0.8, 0.73, 5]: 1 occurrences (0.00%)\n",
      "Creating token mapping for efficient embedding...\n",
      "  - Mapped 11618 unique array tokens to indices 0-11617\n",
      "Remapping dataset to use contiguous indices...\n",
      "  - Remapped 100000 array tokens\n",
      "  - Remapped token range: 0 to 11617\n",
      "Sample of remapped data: tensor([ 8713,  4395, 10242, 10465, 10126, 10462,  4658,   447, 10108,  3191,\n",
      "          217,  3732,  5795, 10558, 10959,  6713,  8180,  3259,  7688,  8425])...\n",
      "\n",
      "--- DYNAMIC VOCABULARY SIZE: 11618 ---\n",
      "This will be used for embedding and output layer dimensions.\n"
     ]
    }
   ],
   "source": [
    "# Generate the raw dataset\n",
    "print(f\"Generating random dataset of {DATASET_SIZE} array tokens...\")\n",
    "raw_train_tokens = generate_random_sequence(DATASET_SIZE)\n",
    "print(f\"Sample of raw data: {raw_train_tokens[:5]}...\")\n",
    "\n",
    "# Analyze vocabulary and get dynamic vocab size\n",
    "VOCAB_SIZE, vocab_stats = analyze_vocabulary(raw_train_tokens)\n",
    "\n",
    "# Create token mapping for efficient embedding\n",
    "token_to_idx, idx_to_token = create_token_mapping(vocab_stats['unique_strings'], raw_train_tokens)\n",
    "\n",
    "# Remap the dataset to use contiguous indices\n",
    "train_data = remap_dataset(raw_train_tokens, token_to_idx)\n",
    "print(f\"Sample of remapped data: {train_data[:20]}...\")\n",
    "\n",
    "# Move the dataset to the selected device\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "print(f\"\\n--- DYNAMIC VOCABULARY SIZE: {VOCAB_SIZE} ---\")\n",
    "print(f\"This will be used for embedding and output layer dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0edbf-bf7a-47d6-a0e0-f7b17b3fdeda",
   "metadata": {},
   "source": [
    "## Function to get batch of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85ee270d-39ee-4325-9cb0-b34f1ea48840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's see an example of a single batch with batch_size=1 and seq_length=5:\n",
      "Source (x): [5468, 220, 1947, 2456, 1908]\n",
      "Target (y): [220, 1947, 2456, 1908, 10315]\n",
      "Notice that the target is the source sequence shifted one position to the right.\n"
     ]
    }
   ],
   "source": [
    "def get_batch(source_data, seq_length, batch_size):\n",
    "    \"\"\"\n",
    "    Generates a batch of source and target sequences for training.\n",
    "    This is the core of how we set up the \"next token prediction\" task.\n",
    "    \"\"\"\n",
    "    # Get the total length of the dataset\n",
    "    num_tokens = len(source_data)\n",
    "    # Generate random starting points for our sequences within the dataset\n",
    "    # We subtract seq_length + 1 to ensure we have a valid target for each sequence\n",
    "    start_indices = torch.randint(0, num_tokens - seq_length - 1, (batch_size,))\n",
    "\n",
    "    # Create the source sequences (input to the model)\n",
    "    # torch.stack builds a new tensor from a list of tensors\n",
    "    x = torch.stack([source_data[i : i + seq_length] for i in start_indices])\n",
    "\n",
    "    # Create the target sequences (what the model should predict)\n",
    "    # The target for each token in the input is the very next token in the sequence.\n",
    "    y = torch.stack([source_data[i + 1 : i + seq_length + 1] for i in start_indices])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "print(f\"\\nLet's see an example of a single batch with batch_size=1 and seq_length=5:\")\n",
    "x_sample, y_sample = get_batch(train_data, 5, 1)\n",
    "print(f\"Source (x): {x_sample.squeeze().tolist()}\")\n",
    "print(f\"Target (y): {y_sample.squeeze().tolist()}\")\n",
    "print(\"Notice that the target is the source sequence shifted one position to the right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef213a-8c60-4bdd-85fc-4a33ca50ce71",
   "metadata": {},
   "source": [
    "## Create positional encoding since transformers see all permutations as same. Need this to make sure sequence is learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f3e131d-5c52-4ca7-9c07-803d6f6ad469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects position information into the token embeddings.\n",
    "    Since the Transformer architecture itself doesn't have a notion of order,\n",
    "    we add these positional encodings to the input embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a matrix for positional encodings of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # Create a position tensor [0, 1, 2, ..., max_len-1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Calculate the division term for the sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices in the array; 2i\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices in the array; 2i+1\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension to the positional encoding matrix\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # Register 'pe' as a buffer. Buffers are part of the model's state,\n",
    "        # but they are not considered model parameters to be trained.\n",
    "        self.register_buffer('pe', pe)\n",
    "        print(\"Initialized PositionalEncoding module.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Add the positional encoding to the input tensor\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5028858-2500-4d50-8692-9d5069b5c617",
   "metadata": {},
   "source": [
    "## The transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff5bc911-c049-46bc-b545-881d5146b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer model for sequence-to-sequence tasks.\n",
    "    In our case, it's used for next-token prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, nhead, d_hid, nlayers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # 1. Token Embedding Layer: Maps input token indices to dense vectors.\n",
    "        self.encoder = nn.Embedding(vocab_size, d_model)\n",
    "        print(f\"Initialized nn.Embedding: maps {vocab_size} tokens to {d_model}-dim vectors.\")\n",
    "\n",
    "        # 2. Positional Encoding: Adds positional information.\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # 3. Transformer Encoder Layers: The core of the model.\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        print(f\"Initialized nn.TransformerEncoder with {nlayers} layers.\")\n",
    "\n",
    "        # 4. Final Linear Layer (Decoder): Maps the Transformer output back to the vocabulary space.\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "        print(f\"Initialized final nn.Linear decoder: maps {d_model}-dim vectors to {vocab_size} (vocab size) logits.\")\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initializes weights for the embedding and linear layers.\"\"\"\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required). Shape: [seq_len, batch_size].\n",
    "            src_mask: the mask for the src sequence (required).\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Inside Model Forward Pass ---\")\n",
    "        print(f\"Input `src` shape: {src.shape} [Sequence Length, Batch Size]\")\n",
    "\n",
    "        # 1. Embed the tokens and scale by sqrt(d_model)\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        print(f\"Shape after Embedding and Scaling: {src.shape} [Seq Len, Batch, Embedding Dim]\")\n",
    "\n",
    "        # 2. Add positional encoding\n",
    "        src = self.pos_encoder(src)\n",
    "        print(f\"Shape after Positional Encoding: {src.shape} [Seq Len, Batch, Embedding Dim]\")\n",
    "\n",
    "        # 3. Pass through the Transformer encoder layers\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        print(f\"Shape after Transformer Encoder: {output.shape} [Seq Len, Batch, Embedding Dim]\")\n",
    "\n",
    "        # 4. Decode the output to get logits for each token in the vocabulary\n",
    "        output = self.decoder(output)\n",
    "        print(f\"Shape after Final Decoder Layer: {output.shape} [Seq Len, Batch, Vocab Size]\")\n",
    "        print(\"--- End of Model Forward Pass ---\\n\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062bd820-7949-49b6-98ab-1c6a473f4235",
   "metadata": {},
   "source": [
    "## Creating mask to prevent the model from seeing future tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e6cff0f-3fd8-4f2a-8302-5aa7a67dab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"\n",
    "    Generates a square causal mask for the sequence.\n",
    "    The masked positions are filled with -inf.\n",
    "    Unmasked positions are 0. This prevents the model from \"cheating\" by\n",
    "    looking at future tokens during training.\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a2c02-c192-4a79-aee1-ca8f21d096d0",
   "metadata": {},
   "source": [
    "## Initialize Model, loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee020b2b-8b0f-46ec-8777-c28ae2db3fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized nn.Embedding: maps 11618 tokens to 256-dim vectors.\n",
      "Initialized PositionalEncoding module.\n",
      "Initialized nn.TransformerEncoder with 4 layers.\n",
      "Initialized final nn.Linear decoder: maps 256-dim vectors to 11618 (vocab size) logits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saahil/projects/MelodyGenerator/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with dynamic vocabulary size: 11618\n",
      "Total model parameters: 9,119,074\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with the dynamically calculated vocab size\n",
    "model = TransformerModel(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, FF_DIM, NUM_ENCODER_LAYERS, DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Model initialized with dynamic vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"Total model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c6a44-ecb4-45a6-8593-77c32f40c1db",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fa4ccd0-0a4d-4859-8713-6fa7b0507d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Epoch 1 ---\n",
      "Shape of data batch fed to model: torch.Size([64, 32])\n",
      "Shape of target batch for loss: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "\n",
      "--- Inside Model Forward Pass ---\n",
      "Input `src` shape: torch.Size([64, 32]) [Sequence Length, Batch Size]\n",
      "Shape after Embedding and Scaling: torch.Size([64, 32, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Positional Encoding: torch.Size([64, 32, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Transformer Encoder: torch.Size([64, 32, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Final Decoder Layer: torch.Size([64, 32, 11618]) [Seq Len, Batch, Vocab Size]\n",
      "--- End of Model Forward Pass ---\n",
      "\n",
      "| epoch   1 |   200/ 1536 batches | lr 0.00100 | ms/batch 27.12 | loss  8.51 | ppl  4988.96\n",
      "| epoch   1 |   400/ 1536 batches | lr 0.00100 | ms/batch 25.83 | loss  5.72 | ppl   303.66\n",
      "| epoch   1 |   600/ 1536 batches | lr 0.00100 | ms/batch 25.87 | loss  3.66 | ppl    38.88\n",
      "| epoch   1 |   800/ 1536 batches | lr 0.00100 | ms/batch 26.02 | loss  2.35 | ppl    10.49\n",
      "| epoch   1 |  1000/ 1536 batches | lr 0.00100 | ms/batch 26.01 | loss  1.49 | ppl     4.43\n",
      "| epoch   1 |  1200/ 1536 batches | lr 0.00100 | ms/batch 26.18 | loss  1.03 | ppl     2.79\n",
      "| epoch   1 |  1400/ 1536 batches | lr 0.00100 | ms/batch 26.10 | loss  0.77 | ppl     2.17\n",
      "--- End of Epoch 1 | Time: 40.86s ---\n",
      "\n",
      "--- Starting Epoch 2 ---\n",
      "Shape of data batch fed to model: torch.Size([64, 32])\n",
      "Shape of target batch for loss: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "| epoch   2 |   200/ 1536 batches | lr 0.00100 | ms/batch 26.33 | loss  0.58 | ppl     1.79\n",
      "| epoch   2 |   400/ 1536 batches | lr 0.00100 | ms/batch 26.33 | loss  0.52 | ppl     1.68\n",
      "| epoch   2 |   600/ 1536 batches | lr 0.00100 | ms/batch 26.26 | loss  0.48 | ppl     1.62\n",
      "| epoch   2 |   800/ 1536 batches | lr 0.00100 | ms/batch 26.25 | loss  0.45 | ppl     1.57\n",
      "| epoch   2 |  1000/ 1536 batches | lr 0.00100 | ms/batch 26.32 | loss  0.42 | ppl     1.53\n",
      "| epoch   2 |  1200/ 1536 batches | lr 0.00100 | ms/batch 26.32 | loss  0.40 | ppl     1.50\n",
      "| epoch   2 |  1400/ 1536 batches | lr 0.00100 | ms/batch 26.32 | loss  0.39 | ppl     1.47\n",
      "--- End of Epoch 2 | Time: 41.08s ---\n",
      "\n",
      "--- Starting Epoch 3 ---\n",
      "Shape of data batch fed to model: torch.Size([64, 32])\n",
      "Shape of target batch for loss: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "| epoch   3 |   200/ 1536 batches | lr 0.00100 | ms/batch 26.67 | loss  0.36 | ppl     1.43\n",
      "| epoch   3 |   400/ 1536 batches | lr 0.00100 | ms/batch 26.89 | loss  0.34 | ppl     1.41\n",
      "| epoch   3 |   600/ 1536 batches | lr 0.00100 | ms/batch 26.69 | loss  0.33 | ppl     1.40\n",
      "| epoch   3 |   800/ 1536 batches | lr 0.00100 | ms/batch 26.83 | loss  0.32 | ppl     1.38\n",
      "| epoch   3 |  1000/ 1536 batches | lr 0.00100 | ms/batch 26.85 | loss  0.32 | ppl     1.37\n",
      "| epoch   3 |  1200/ 1536 batches | lr 0.00100 | ms/batch 27.37 | loss  0.31 | ppl     1.36\n",
      "| epoch   3 |  1400/ 1536 batches | lr 0.00100 | ms/batch 27.12 | loss  0.30 | ppl     1.36\n",
      "--- End of Epoch 3 | Time: 42.02s ---\n",
      "\n",
      "--- Starting Epoch 4 ---\n",
      "Shape of data batch fed to model: torch.Size([64, 32])\n",
      "Shape of target batch for loss: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "| epoch   4 |   200/ 1536 batches | lr 0.00100 | ms/batch 26.92 | loss  0.30 | ppl     1.35\n",
      "| epoch   4 |   400/ 1536 batches | lr 0.00100 | ms/batch 27.03 | loss  0.29 | ppl     1.33\n",
      "| epoch   4 |   600/ 1536 batches | lr 0.00100 | ms/batch 26.93 | loss  0.29 | ppl     1.33\n",
      "| epoch   4 |   800/ 1536 batches | lr 0.00100 | ms/batch 27.07 | loss  0.28 | ppl     1.33\n",
      "| epoch   4 |  1000/ 1536 batches | lr 0.00100 | ms/batch 27.05 | loss  0.28 | ppl     1.32\n",
      "| epoch   4 |  1200/ 1536 batches | lr 0.00100 | ms/batch 27.15 | loss  0.27 | ppl     1.32\n",
      "| epoch   4 |  1400/ 1536 batches | lr 0.00100 | ms/batch 27.32 | loss  0.28 | ppl     1.32\n",
      "--- End of Epoch 4 | Time: 42.31s ---\n",
      "\n",
      "--- Starting Epoch 5 ---\n",
      "Shape of data batch fed to model: torch.Size([64, 32])\n",
      "Shape of target batch for loss: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "| epoch   5 |   200/ 1536 batches | lr 0.00100 | ms/batch 27.38 | loss  0.27 | ppl     1.31\n",
      "| epoch   5 |   400/ 1536 batches | lr 0.00100 | ms/batch 27.20 | loss  0.27 | ppl     1.30\n",
      "| epoch   5 |   600/ 1536 batches | lr 0.00100 | ms/batch 27.07 | loss  0.27 | ppl     1.30\n",
      "| epoch   5 |   800/ 1536 batches | lr 0.00100 | ms/batch 27.13 | loss  0.26 | ppl     1.30\n",
      "| epoch   5 |  1000/ 1536 batches | lr 0.00100 | ms/batch 27.30 | loss  0.26 | ppl     1.29\n",
      "| epoch   5 |  1200/ 1536 batches | lr 0.00100 | ms/batch 27.05 | loss  0.25 | ppl     1.29\n",
      "| epoch   5 |  1400/ 1536 batches | lr 0.00100 | ms/batch 27.34 | loss  0.25 | ppl     1.29\n",
      "--- End of Epoch 5 | Time: 42.50s ---\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    \"\"\"Defines the training loop for one epoch.\"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    # Generate the causal mask. It's the same for all sequences of the same length.\n",
    "    src_mask = generate_square_subsequent_mask(SEQUENCE_LENGTH).to(device)\n",
    "\n",
    "    # Calculate the number of batches in one epoch\n",
    "    num_batches = len(train_data) // (SEQUENCE_LENGTH * BATCH_SIZE)\n",
    "\n",
    "    print(f\"\\n--- Starting Epoch {epoch} ---\")\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1 - SEQUENCE_LENGTH, SEQUENCE_LENGTH)):\n",
    "        # Get a batch of data\n",
    "        data, targets = get_batch(train_data, SEQUENCE_LENGTH, BATCH_SIZE)\n",
    "        \n",
    "        # The model expects inputs of shape [sequence_length, batch_size]\n",
    "        # Our get_batch function returns [batch_size, sequence_length], so we permute it.\n",
    "        data = data.permute(1, 0)\n",
    "        targets = targets.permute(1, 0)\n",
    "        \n",
    "        # The first time through, print shapes to be extra clear\n",
    "        if batch == 0:\n",
    "            print(f\"Shape of data batch fed to model: {data.shape}\")\n",
    "            print(f\"Shape of target batch for loss: {targets.shape}\")\n",
    "            print(f\"Shape of causal mask: {src_mask.shape}\")\n",
    "            print(\"Starting batch iterations...\")\n",
    "\n",
    "        optimizer.zero_grad() # Reset gradients\n",
    "\n",
    "        # This is where we stop printing the forward pass details to avoid clutter\n",
    "        # We'll only do it once during the prediction phase.\n",
    "        if batch == 0 and epoch == 1:\n",
    "            output = model(data, src_mask) # Run the forward pass\n",
    "        else:\n",
    "            # Temporarily disable the print statements in the forward pass\n",
    "            # for cleaner training logs.\n",
    "            _print = __builtins__.print\n",
    "            __builtins__.print = lambda *args, **kwargs: None\n",
    "            output = model(data, src_mask)\n",
    "            __builtins__.print = _print\n",
    "\n",
    "        # Reshape the output and targets for the loss function\n",
    "        # The loss function expects [Batch * SeqLen, VocabSize] and [Batch * SeqLen]\n",
    "        loss = criterion(output.view(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "\n",
    "        loss.backward() # Compute gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # Prevent exploding gradients\n",
    "        optimizer.step() # Update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Log progress\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / LOG_INTERVAL\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches * BATCH_SIZE:5d} batches | '\n",
    "                  f'lr {lr:02.5f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(epoch)\n",
    "    print(f'--- End of Epoch {epoch} | Time: {(time.time() - epoch_start_time):.2f}s ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674eeccc-a469-4c0a-9b98-5f6e5c64f94a",
   "metadata": {},
   "source": [
    "## Generating new sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0903e463-cc06-4f5d-9a80-169e5d157ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, seed_sequence, max_len=50, idx_to_token=None):\n",
    "    \"\"\"\n",
    "    Generates a sequence token by token based on a seed.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    print(f\"Seed sequence (indices): {seed_sequence}\")\n",
    "    if idx_to_token:\n",
    "        original_tokens = [idx_to_token[idx] for idx in seed_sequence]\n",
    "        print(f\"Seed sequence (original array tokens): {original_tokens}\")\n",
    "    \n",
    "    # Convert the seed sequence (list of ints) to a tensor\n",
    "    input_tensor = torch.tensor(seed_sequence, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    \n",
    "    generated_sequence = seed_sequence.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_len):\n",
    "            print(f\"\\n--- Prediction step {step + 1} ---\")\n",
    "            \n",
    "            # Create the causal mask for the current sequence length\n",
    "            current_seq_len = input_tensor.size(0)\n",
    "            mask = generate_square_subsequent_mask(current_seq_len).to(device)\n",
    "            \n",
    "            # Get the model's output\n",
    "            if step == 0:\n",
    "                output = model(input_tensor, mask)\n",
    "            else:\n",
    "                _print = __builtins__.print\n",
    "                __builtins__.print = lambda *args, **kwargs: None\n",
    "                output = model(input_tensor, mask)\n",
    "                __builtins__.print = _print\n",
    "            \n",
    "            # Get the last token's logits\n",
    "            last_token_logits = output[-1, 0, :]\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample the next token\n",
    "            next_token = torch.multinomial(probabilities, 1).item()\n",
    "            \n",
    "            predicted_original = idx_to_token[next_token] if idx_to_token else next_token\n",
    "            print(f\"Model predicted next token: {next_token} (original array: {predicted_original})\")\n",
    "            \n",
    "            # Append the predicted token to our sequence\n",
    "            generated_sequence.append(next_token)\n",
    "            \n",
    "            # Create the new input for the next iteration\n",
    "            input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=device)], dim=0)\n",
    "\n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014cc7a-99f7-42eb-9351-2da8a4787f31",
   "metadata": {},
   "source": [
    "## Generating new sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0067d05-36c2-40cf-8636-b8e8c411a170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Final Result ---\n",
      "Seed sequence (indices): [0, 1, 2, 3, 4]\n",
      "Seed sequence (original array tokens): [[6, 0.83, 0.53, 9], [6, 0.97, 0.75, 8], [2, 0.43, 0.45, 2], [3, 0.35, 0.39, 4], [0, 0.05, 0.25, 0]]\n",
      "\n",
      "--- Prediction step 1 ---\n",
      "\n",
      "--- Inside Model Forward Pass ---\n",
      "Input `src` shape: torch.Size([5, 1]) [Sequence Length, Batch Size]\n",
      "Shape after Embedding and Scaling: torch.Size([5, 1, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Positional Encoding: torch.Size([5, 1, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Transformer Encoder: torch.Size([5, 1, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Final Decoder Layer: torch.Size([5, 1, 11618]) [Seq Len, Batch, Vocab Size]\n",
      "--- End of Model Forward Pass ---\n",
      "\n",
      "Model predicted next token: 5173 (original array: [0, 0.22, 0.13, 1])\n",
      "\n",
      "--- Prediction step 2 ---\n",
      "Model predicted next token: 2854 (original array: [0, 0.25, 0.01, 0])\n",
      "\n",
      "--- Prediction step 3 ---\n",
      "Model predicted next token: 1728 (original array: [0, 0.03, 0.08, 1])\n",
      "\n",
      "--- Prediction step 4 ---\n",
      "Model predicted next token: 1695 (original array: [1, 0.01, 0.08, 0])\n",
      "\n",
      "--- Prediction step 5 ---\n",
      "Model predicted next token: 8585 (original array: [0, 0.07, 0.04, 1])\n",
      "\n",
      "--- Prediction step 6 ---\n",
      "Model predicted next token: 8880 (original array: [1, 0.02, 0.04, 1])\n",
      "\n",
      "--- Prediction step 7 ---\n",
      "Model predicted next token: 7282 (original array: [2, 0.39, 0.38, 4])\n",
      "\n",
      "--- Prediction step 8 ---\n",
      "Model predicted next token: 5358 (original array: [0, 0.17, 0.04, 1])\n",
      "\n",
      "--- Prediction step 9 ---\n",
      "Model predicted next token: 3433 (original array: [0, 0.02, 0.04, 1])\n",
      "\n",
      "--- Prediction step 10 ---\n",
      "Model predicted next token: 5484 (original array: [1, 0.02, 0.18, 1])\n",
      "\n",
      "--- Prediction step 11 ---\n",
      "Model predicted next token: 2763 (original array: [3, 0.26, 0.48, 4])\n",
      "\n",
      "--- Prediction step 12 ---\n",
      "Model predicted next token: 2862 (original array: [1, 0.08, 0.16, 0])\n",
      "\n",
      "--- Prediction step 13 ---\n",
      "Model predicted next token: 10217 (original array: [0, 0.14, 0.0, 1])\n",
      "\n",
      "--- Prediction step 14 ---\n",
      "Model predicted next token: 7583 (original array: [1, 0.24, 0.12, 1])\n",
      "\n",
      "--- Prediction step 15 ---\n",
      "Model predicted next token: 1324 (original array: [3, 0.35, 0.44, 4])\n",
      "\n",
      "--- Prediction step 16 ---\n",
      "Model predicted next token: 2173 (original array: [2, 0.28, 0.33, 4])\n",
      "\n",
      "--- Prediction step 17 ---\n",
      "Model predicted next token: 7696 (original array: [0, 0.13, 0.01, 1])\n",
      "\n",
      "--- Prediction step 18 ---\n",
      "Model predicted next token: 7929 (original array: [1, 0.22, 0.15, 0])\n",
      "\n",
      "--- Prediction step 19 ---\n",
      "Model predicted next token: 10938 (original array: [0, 0.15, 0.11, 0])\n",
      "\n",
      "--- Prediction step 20 ---\n",
      "Model predicted next token: 3149 (original array: [0, 0.02, 0.14, 0])\n",
      "Original Seed (indices): [0, 1, 2, 3, 4]\n",
      "Original Seed (original array tokens): [[6, 0.83, 0.53, 9], [6, 0.97, 0.75, 8], [2, 0.43, 0.45, 2], [3, 0.35, 0.39, 4], [0, 0.05, 0.25, 0]]\n",
      "Generated Sequence (indices): [0, 1, 2, 3, 4, 5173, 2854, 1728, 1695, 8585, 8880, 7282, 5358, 3433, 5484, 2763, 2862, 10217, 7583, 1324, 2173, 7696, 7929, 10938, 3149]\n",
      "Generated Sequence (original array tokens): [[6, 0.83, 0.53, 9], [6, 0.97, 0.75, 8], [2, 0.43, 0.45, 2], [3, 0.35, 0.39, 4], [0, 0.05, 0.25, 0], [0, 0.22, 0.13, 1], [0, 0.25, 0.01, 0], [0, 0.03, 0.08, 1], [1, 0.01, 0.08, 0], [0, 0.07, 0.04, 1], [1, 0.02, 0.04, 1], [2, 0.39, 0.38, 4], [0, 0.17, 0.04, 1], [0, 0.02, 0.04, 1], [1, 0.02, 0.18, 1], [3, 0.26, 0.48, 4], [1, 0.08, 0.16, 0], [0, 0.14, 0.0, 1], [1, 0.24, 0.12, 1], [3, 0.35, 0.44, 4], [2, 0.28, 0.33, 4], [0, 0.13, 0.01, 1], [1, 0.22, 0.15, 0], [0, 0.15, 0.11, 0], [0, 0.02, 0.14, 0]]\n",
      "\n",
      "Training and prediction complete!\n",
      "Final vocabulary size used: 11618\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n--- Final Result ---\")\n",
    "seed_indices = [0, 1, 2, 3, 4]  # These are indices in our remapped space\n",
    "predicted_sequence = predict(model, seed_indices, max_len=20, idx_to_token=idx_to_token)\n",
    "\n",
    "print(f\"Original Seed (indices): {seed_indices}\")\n",
    "print(f\"Original Seed (original array tokens): {[idx_to_token[idx] for idx in seed_indices]}\")\n",
    "print(f\"Generated Sequence (indices): {predicted_sequence}\")\n",
    "print(f\"Generated Sequence (original array tokens): {[idx_to_token[idx] for idx in predicted_sequence]}\")\n",
    "print(\"\\nTraining and prediction complete!\")\n",
    "print(f\"Final vocabulary size used: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162416b-e7c4-4765-9a5a-0011ead9dd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
