{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ebb682-d848-4afd-b149-930222338243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a686de-bce9-4fe3-822e-fee2368a08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd3ccd71-3981-40a8-bad4-966c75c563ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=10, device='cuda'):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.sequence_length]\n",
    "        y = self.data[idx + self.sequence_length]\n",
    "        # Move to device if specified\n",
    "        if self.device == 'cuda':\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1974527a-e99d-4071-aacb-373b00a1fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9627756-242a-4f00-ab15-e37c1c20624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=4, d_model=64, nhead=4, num_layers=3, \n",
    "                 dim_feedforward=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.output_projection = nn.Linear(d_model, input_dim)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.input_projection(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        transformer_out = self.transformer(x)\n",
    "        last_token = transformer_out[:, -1, :]\n",
    "        output = self.output_projection(last_token)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62a7a01-9146-4855-a164-d079054f4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyGenerator:\n",
    "    def __init__(self, model_params=None, force_cpu=False):\n",
    "        self.model_params = model_params or {\n",
    "            'd_model': 128,\n",
    "            'nhead': 8,\n",
    "            'num_layers': 6,\n",
    "            'dim_feedforward': 512,\n",
    "            'dropout': 0.1\n",
    "        }\n",
    "        \n",
    "        # CUDA setup with fallback\n",
    "        if force_cpu:\n",
    "            self.device = torch.device('cpu')\n",
    "            print(\"Forced to use CPU\")\n",
    "        else:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.model = None\n",
    "        self.data_stats = None\n",
    "        \n",
    "        # Enable CUDA optimizations if available\n",
    "        if torch.cuda.is_available() and not force_cpu:\n",
    "            torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes\n",
    "            torch.backends.cudnn.deterministic = False  # Allow non-deterministic algorithms for speed\n",
    "    \n",
    "    def normalize_data(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        self.data_stats = {\n",
    "            'mean': np.mean(data, axis=0),\n",
    "            'std': np.std(data, axis=0) + 1e-8\n",
    "        }\n",
    "        normalized_data = (data - self.data_stats['mean']) / self.data_stats['std']\n",
    "        return normalized_data.tolist()\n",
    "    \n",
    "    def denormalize_data(self, data):\n",
    "        if self.data_stats is None:\n",
    "            return data\n",
    "        \n",
    "        data = np.array(data)\n",
    "        denormalized = data * self.data_stats['std'] + self.data_stats['mean']\n",
    "        denormalized[:, [0, 3]] = np.round(denormalized[:, [0, 3]])\n",
    "        denormalized[:, [1, 2]] = np.round(denormalized[:, [1, 2]], 2)\n",
    "        return denormalized.tolist()\n",
    "    \n",
    "    def train(self, data, sequence_length=10, epochs=100, batch_size=32, \n",
    "              learning_rate=0.001, validation_split=0.2, use_amp=True):\n",
    "        \"\"\"\n",
    "        Train with CUDA optimizations\n",
    "        \n",
    "        Args:\n",
    "            use_amp: Use Automatic Mixed Precision for faster training on modern GPUs\n",
    "        \"\"\"\n",
    "        print(f\"Training on {len(data)} samples using {self.device}...\")\n",
    "        \n",
    "        # CUDA memory management\n",
    "        if self.device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()  # Clear cache\n",
    "            print(f\"GPU Memory before training: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        \n",
    "        # Normalize data\n",
    "        normalized_data = self.normalize_data(data)\n",
    "        \n",
    "        # Split data\n",
    "        split_idx = int(len(normalized_data) * (1 - validation_split))\n",
    "        train_data = normalized_data[:split_idx]\n",
    "        val_data = normalized_data[split_idx:]\n",
    "        \n",
    "        # Create datasets - pin_memory for faster GPU transfer\n",
    "        pin_memory = self.device.type == 'cuda'\n",
    "        \n",
    "        train_dataset = MelodyDataset(train_data, sequence_length, device='cpu')  # Keep on CPU for DataLoader\n",
    "        val_dataset = MelodyDataset(val_data, sequence_length, device='cpu')\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            pin_memory=pin_memory,\n",
    "            num_workers=2 if self.device.type == 'cuda' else 0  # Parallel loading for GPU\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            pin_memory=pin_memory,\n",
    "            num_workers=2 if self.device.type == 'cuda' else 0\n",
    "        )\n",
    "        \n",
    "        # Initialize model and move to device\n",
    "        self.model = MelodyTransformer(**self.model_params).to(self.device)\n",
    "        \n",
    "        # Enable compilation for PyTorch 2.0+ (massive speedup on GPU)\n",
    "        if hasattr(torch, 'compile') and self.device.type == 'cuda':\n",
    "            try:\n",
    "                self.model = torch.compile(self.model)\n",
    "                print(\"Model compiled for faster execution!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Compilation failed (using uncompiled model): {e}\")\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "        \n",
    "        # Mixed precision training for modern GPUs\n",
    "        scaler = torch.amp.GradScaler('cuda') if use_amp and self.device.type == 'cuda' else None\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                # Move data to device\n",
    "                batch_x, batch_y = batch_x.to(self.device, non_blocking=True), batch_y.to(self.device, non_blocking=True)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass with optional mixed precision\n",
    "                if scaler is not None:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        output = self.model(batch_x)\n",
    "                        loss = criterion(output, batch_y)\n",
    "                    \n",
    "                    # Backward pass with gradient scaling\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    output = self.model(batch_x)\n",
    "                    loss = criterion(output, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(self.device, non_blocking=True), batch_y.to(self.device, non_blocking=True)\n",
    "                    \n",
    "                    if scaler is not None:\n",
    "                        with torch.amp.autocast('cuda'):\n",
    "                            output = self.model(batch_x)\n",
    "                            loss = criterion(output, batch_y)\n",
    "                    else:\n",
    "                        output = self.model(batch_x)\n",
    "                        loss = criterion(output, batch_y)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                memory_info = f\", GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\" if self.device.type == 'cuda' else \"\"\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}{memory_info}')\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "        # Clear cache after training\n",
    "        if self.device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def generate(self, seed_sequence, num_generate=10):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Normalize seed sequence\n",
    "        seed_normalized = []\n",
    "        for tuple_data in seed_sequence:\n",
    "            normalized_tuple = ((np.array(tuple_data) - self.data_stats['mean']) / \n",
    "                              self.data_stats['std']).tolist()\n",
    "            seed_normalized.append(normalized_tuple)\n",
    "        \n",
    "        generated = []\n",
    "        current_sequence = seed_normalized.copy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_generate):\n",
    "                # Convert to tensor and move to device\n",
    "                input_tensor = torch.tensor([current_sequence], dtype=torch.float32).to(self.device)\n",
    "                \n",
    "                # Generate next tuple\n",
    "                output = self.model(input_tensor)\n",
    "                next_tuple = output.cpu().numpy()[0].tolist()\n",
    "                \n",
    "                generated.append(next_tuple)\n",
    "                current_sequence = current_sequence[1:] + [next_tuple]\n",
    "        \n",
    "        # Denormalize generated data\n",
    "        generated_denormalized = self.denormalize_data(generated)\n",
    "        return generated_denormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecb47a13-fc84-4710-bb9c-78ac8b3c1e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1724 samples\n",
      "Using device: cuda\n",
      "Training on 1724 samples using cuda...\n",
      "GPU Memory before training: 0.00 GB\n",
      "Model compiled for faster execution!\n",
      "Epoch [10/100], Train Loss: 0.167741, Val Loss: 0.055143, GPU Memory: 0.04 GB\n",
      "Epoch [20/100], Train Loss: 0.114650, Val Loss: 0.085822, GPU Memory: 0.04 GB\n",
      "Epoch [30/100], Train Loss: 0.095802, Val Loss: 0.065267, GPU Memory: 0.04 GB\n",
      "Epoch [40/100], Train Loss: 0.087101, Val Loss: 0.099337, GPU Memory: 0.04 GB\n",
      "Epoch [50/100], Train Loss: 0.082106, Val Loss: 0.103828, GPU Memory: 0.04 GB\n",
      "Epoch [60/100], Train Loss: 0.076499, Val Loss: 0.112738, GPU Memory: 0.04 GB\n",
      "Epoch [70/100], Train Loss: 0.079612, Val Loss: 0.107870, GPU Memory: 0.04 GB\n",
      "Epoch [80/100], Train Loss: 0.072470, Val Loss: 0.119233, GPU Memory: 0.04 GB\n",
      "Epoch [90/100], Train Loss: 0.069350, Val Loss: 0.121909, GPU Memory: 0.04 GB\n",
      "Epoch [100/100], Train Loss: 0.068723, Val Loss: 0.125661, GPU Memory: 0.04 GB\n",
      "Training completed!\n",
      "\n",
      "Generated tuples:\n",
      "  1: [81.0, 42.77, 40.06, 100.0]\n",
      "  2: [81.0, 42.34, 40.04, 100.0]\n",
      "  3: [82.0, 42.09, 39.69, 100.0]\n",
      "  4: [83.0, 41.88, 39.52, 100.0]\n",
      "  5: [82.0, 41.69, 39.45, 100.0]\n",
      "  6: [82.0, 41.46, 39.23, 100.0]\n",
      "  7: [83.0, 41.45, 39.26, 100.0]\n",
      "  8: [82.0, 41.37, 39.2, 100.0]\n",
      "  9: [83.0, 41.46, 39.32, 100.0]\n",
      "  10: [83.0, 41.58, 39.46, 100.0]\n",
      "  11: [83.0, 41.97, 39.88, 100.0]\n",
      "  12: [83.0, 42.42, 40.32, 100.0]\n",
      "  13: [84.0, 42.95, 40.87, 100.0]\n",
      "  14: [84.0, 43.61, 41.5, 100.0]\n",
      "  15: [84.0, 44.35, 42.23, 100.0]\n",
      "  16: [84.0, 45.22, 43.07, 100.0]\n",
      "  17: [84.0, 46.16, 43.98, 100.0]\n",
      "  18: [84.0, 47.17, 45.02, 100.0]\n",
      "  19: [84.0, 48.28, 46.13, 100.0]\n",
      "  20: [85.0, 49.46, 47.32, 100.0]\n"
     ]
    }
   ],
   "source": [
    "instrument = 0\n",
    "\n",
    "# Load your data\n",
    "with open(f'raw_data/{instrument}.json', 'r') as file:\n",
    "    loaded_data = json.load(file)\n",
    "\n",
    "print(f\"Loaded {len(loaded_data)} samples\")\n",
    "\n",
    "# Create CUDA-enabled generator\n",
    "generator = MelodyGenerator()\n",
    "\n",
    "# Train the model with CUDA optimizations\n",
    "train_losses, val_losses = generator.train(\n",
    "    data=loaded_data,\n",
    "    sequence_length=10,\n",
    "    epochs=100,\n",
    "    batch_size=64,  # Larger batch size for GPU efficiency\n",
    "    learning_rate=1e-4,\n",
    "    use_amp=True  # Enable mixed precision for speed\n",
    ")\n",
    "\n",
    "# Generate new patterns\n",
    "seed = loaded_data[:10]  # Use first 10 tuples as seed\n",
    "generated_tuples = generator.generate(seed, num_generate=20)\n",
    "\n",
    "print(\"\\nGenerated tuples:\")\n",
    "for i, tuple_data in enumerate(generated_tuples):\n",
    "    print(f\"  {i+1}: {tuple_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c1be4-8de2-4862-9e2c-8ac3e70b92c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
