{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380d84a4-6ec8-4945-8252-40582c5b8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pretty_midi\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8124a6-a205-4e11-9baa-bc765cac0cf3",
   "metadata": {},
   "source": [
    "### Loading MIDI Token Data\n",
    "This section of the code is responsible for loading the tokenized MIDI data, which represents musical sequences for instrument 29 (electric guitar, as per the MIDI General Standard). The data is stored in a JSON file located in the raw_data directory. The code uses the pathlib library to handle file paths in a platform-independent manner and the json library to parse the JSON file into a Python object, preparing the tokenized data for further processing in the melody transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6106a633-5bfd-4001-ae89-63c657e4ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the tokenized MIDI data\n",
    "# Path(\"raw_data\") creates a directory object, and / \"29.json\" appends the file name\n",
    "file_path = Path(\"raw_data\") / \"29.json\"\n",
    "\n",
    "# Load the data\n",
    "# Open the JSON file in read mode (\"r\") and parse its contents into a Python object\n",
    "with file_path.open(\"r\") as f:\n",
    "    tokens = json.load(f)  # tokens now contains the deserialized JSON data (e.g., list or dict of MIDI tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326efbd1-e5a8-4c6f-9c3b-81ceb8bf845b",
   "metadata": {},
   "source": [
    "### Device Configuration for Model Training\n",
    "This section configures the computational device for training the melody transformer model. It checks for the availability of a CUDA-enabled GPU using the PyTorch library and selects it as the primary device if available; otherwise, it defaults to the CPU. This ensures the model can leverage GPU acceleration for faster training when possible, while maintaining compatibility with CPU-only environments. The chosen device is printed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3795d8e-3da8-45f1-bf3d-a837679c2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Select device: use CUDA (GPU) if available, otherwise fall back to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the selected device for confirmation\n",
    "print(f\"Using device: {device}\")  # Outputs \"cuda\" if GPU is available, else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd853d02-bdaf-4221-b5b4-90ec7a8864c8",
   "metadata": {},
   "source": [
    "### Hyperparameter Configuration for the Melody Transformer\n",
    "\n",
    "This section defines the key hyperparameters for the melody transformer model, which are critical for configuring its architecture and training behavior. These parameters include the dimensionality of token embeddings, the number of attention heads in the multi-head attention mechanism, the number of stacked transformer encoder layers, the size of the feed-forward network within each layer, and the dropout probability for regularization. These values are chosen to balance model capacity, computational efficiency, and generalization when generating musical sequences from MIDI data for instrument 29 (electric guitar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c71acc-2926-4d46-a4c4-1f34e0ef15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensionality of the token embeddings. This is the size of the vector\n",
    "# that will represent each token in the input sequence, capturing its semantic features.\n",
    "EMBEDDING_DIM = 64  # Set to 256, a common choice for moderate model capacity\n",
    "\n",
    "# The number of attention heads in the multi-head attention mechanism.\n",
    "# Each head processes a portion of the embedding, and EMBEDDING_DIM must be divisible by NUM_HEADS.\n",
    "NUM_HEADS = 4  # 8 heads allow parallel attention computations; 256 / 8 = 32 dimensions per head\n",
    "\n",
    "# The number of Transformer encoder layers to stack.\n",
    "# More layers increase model depth and capacity but also computational cost.\n",
    "NUM_ENCODER_LAYERS = 2  # 4 layers provide a balance between expressiveness and efficiency\n",
    "\n",
    "# The dimension of the feed-forward network within each Transformer layer.\n",
    "# This defines the size of the intermediate layer in the feed-forward subnetwork.\n",
    "FF_DIM = 256  # 1024 units for a larger capacity in the feed-forward network\n",
    "\n",
    "# The dropout probability to be applied in the model.\n",
    "# Dropout helps prevent overfitting by randomly setting a fraction of activations to zero during training.\n",
    "DROPOUT = 0.1  # 10% dropout for regularization, a standard value for transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53d4bb-34f3-46dd-a189-9e9e10f32377",
   "metadata": {},
   "source": [
    "### Training Hyperparameter Configuration\n",
    "This section specifies the hyperparameters governing the training process of the melody transformer model. These include the batch size, which determines the number of sequences processed in parallel; the sequence length, which sets the context window for backpropagation; the number of training epochs, which defines how many times the model iterates over the dataset; the learning rate, which controls the step size for weight updates; and the logging interval, which dictates how frequently training progress is reported. These parameters are carefully selected to ensure effective training of the model on MIDI data for instrument 29 (electric guitar), balancing computational efficiency and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b462a5c3-87b6-4597-af0f-2fdfd113ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of independent sequences to process in parallel.\n",
    "# Larger batch sizes can improve training stability but require more memory.\n",
    "BATCH_SIZE = 32  # 32 sequences per batch, suitable for moderate GPU memory\n",
    "\n",
    "# The length of the subsequences to be used for training. This is also known\n",
    "# as the context window or \"backpropagation through time\" (BPTT) length.\n",
    "# Shorter sequences reduce memory usage but may limit the model's ability to capture long-term dependencies.\n",
    "SEQUENCE_LENGTH = 64  # 64 tokens, appropriate for capturing musical patterns in MIDI data\n",
    "\n",
    "# The number of epochs to train the model for.\n",
    "# Each epoch represents one full pass through the training dataset.\n",
    "EPOCHS = 5  # 5 epochs, a reasonable starting point for convergence on MIDI data\n",
    "\n",
    "# The learning rate for the optimizer.\n",
    "# Controls the step size for updating model weights during optimization.\n",
    "LEARNING_RATE = 0.001  # 0.001 is a standard learning rate for transformer models with Adam optimizer\n",
    "\n",
    "# How often to log training progress (in batches).\n",
    "# Logging provides insights into training dynamics without excessive output.\n",
    "LOG_INTERVAL = 200  # Log progress every 200 batches for monitoring training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e506253-ab11-4fcf-ab73-d3895dcc126a",
   "metadata": {},
   "source": [
    "### Token Conversion to String Representations\n",
    "This section defines a utility function, tokens_to_strings, which converts an array of tokenized MIDI data into a list of string representations. This function is essential for processing or analyzing the tokenized MIDI sequences (representing musical events for instrument 29, electric guitar) in a human-readable format, facilitating tasks such as comparison, debugging, or logging during the training or evaluation of the melody transformer model. The function ensures compatibility with downstream processes that may require string-based token representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d150251f-a141-4cc7-b3da-f5dacc691db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_strings(tokens):\n",
    "    \"\"\"Convert array tokens to string representations for comparison.\n",
    "    \n",
    "    Args:\n",
    "        tokens: A list or array of tokens, typically integers or symbols representing MIDI events.\n",
    "    \n",
    "    Returns:\n",
    "        A list of strings, where each string is the string representation of a token.\n",
    "    \"\"\"\n",
    "    # Convert each token in the input array to its string representation using a list comprehension\n",
    "    return [str(token) for token in tokens]  # Returns a list of stringified tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f0ac4-bd6d-453d-9cd7-877b527bf870",
   "metadata": {},
   "source": [
    "### Vocabulary Analysis of MIDI Token Dataset\n",
    "This section implements the analyze_vocabulary function, which performs a statistical analysis of the tokenized MIDI dataset for instrument 29 (electric guitar). The function calculates key metrics, such as the vocabulary size (number of unique tokens), total token count, and average token frequency, to provide insights into the dataset's characteristics. It also identifies and displays the most and least frequent tokens, along with their occurrence percentages, to highlight common and rare musical events. These statistics are crucial for understanding the dataset's complexity and informing the design of the melody transformer model, particularly for tasks like embedding layer configuration and handling rare tokens during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "298fce32-e454-49ef-a925-fb641fb4b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_vocabulary(data_tokens):\n",
    "    \"\"\"\n",
    "    Analyzes the dataset to determine vocabulary characteristics.\n",
    "    Returns vocabulary size and other statistics.\n",
    "    \n",
    "    Args:\n",
    "        data_tokens: A list or array of tokens representing MIDI events.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (vocab_size, stats_dict)\n",
    "            - vocab_size: Integer, the number of unique tokens in the dataset.\n",
    "            - stats_dict: Dictionary containing unique_strings, token_counts, and total_tokens.\n",
    "    \"\"\"\n",
    "    # Print a message to indicate the start of vocabulary analysis\n",
    "    print(\"Analyzing vocabulary from the dataset...\")\n",
    "    \n",
    "    # Convert tokens to string representations for consistent comparison\n",
    "    # Uses the tokens_to_strings function defined earlier\n",
    "    token_strings = tokens_to_strings(data_tokens)\n",
    "    \n",
    "    # Create a set of unique token strings to compute vocabulary size\n",
    "    unique_strings = list(set(token_strings))\n",
    "    vocab_size = len(unique_strings)  # Number of unique tokens\n",
    "    \n",
    "    # Count the frequency of each token in the dataset\n",
    "    token_counts = {}\n",
    "    for token_str in token_strings:\n",
    "        # Increment the count for the token, initializing to 0 if not present\n",
    "        token_counts[token_str] = token_counts.get(token_str, 0) + 1\n",
    "    \n",
    "    # Calculate the total number of tokens in the dataset\n",
    "    total_tokens = len(data_tokens)\n",
    "    \n",
    "    # Sort tokens by frequency in descending order for analysis\n",
    "    sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Print summary statistics for the vocabulary\n",
    "    print(f\"Vocabulary Analysis Results:\")\n",
    "    print(f\"  - Total unique tokens (vocab size): {vocab_size}\")\n",
    "    print(f\"  - Total tokens in dataset: {total_tokens}\")\n",
    "    print(f\"  - Average token frequency: {total_tokens / vocab_size:.2f}\")\n",
    "    \n",
    "    # Display the top 10 most frequent tokens and their percentages\n",
    "    print(f\"  - Top 10 most frequent tokens:\")\n",
    "    for i in range(min(10, len(sorted_tokens))):\n",
    "        token_str, count = sorted_tokens[i]\n",
    "        percentage = (count / total_tokens) * 100  # Calculate percentage of total tokens\n",
    "        print(f\"    {token_str}: {count} occurrences ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Display up to 5 of the least frequent tokens and their percentages\n",
    "    print(f\"  - Some rare tokens (bottom 5):\")\n",
    "    for i in range(max(0, len(sorted_tokens) - 5), len(sorted_tokens)):\n",
    "        token_str, count = sorted_tokens[i]\n",
    "        percentage = (count / total_tokens) * 100  # Calculate percentage of total tokens\n",
    "        print(f\"    {token_str}: {count} occurrences ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Return the vocabulary size and a dictionary of statistics for further use\n",
    "    return vocab_size, {\n",
    "        'unique_strings': unique_strings,  # List of unique token strings\n",
    "        'token_counts': token_counts,      # Dictionary of token frequencies\n",
    "        'total_tokens': total_tokens       # Total number of tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9efac-23d6-4a71-ac00-7cdd31ac0281",
   "metadata": {},
   "source": [
    "### Token Mapping for Efficient Embedding\n",
    "This section defines the create_token_mapping function, which generates bidirectional mappings between the tokenized MIDI data and contiguous integer indices for instrument 29 (electric guitar). The function creates two dictionaries: one mapping token strings to indices (token_to_idx) and another mapping indices back to the original tokens (idx_to_token). This mapping is essential for converting the dataset's tokens into a format suitable for the melody transformer’s embedding layer, ensuring efficient processing and compatibility with the model’s input requirements. The function also provides feedback on the number of unique tokens mapped, aiding in debugging and verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2343908-9197-40b8-ac12-0421437e01ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_mapping(unique_strings, original_tokens):\n",
    "    \"\"\"\n",
    "    Creates a mapping from original array tokens to contiguous indices.\n",
    "    \n",
    "    Args:\n",
    "        unique_strings: List of unique token strings from the dataset.\n",
    "        original_tokens: List or array of original tokens from the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (token_to_idx, idx_to_token)\n",
    "            - token_to_idx: Dictionary mapping token strings to contiguous indices.\n",
    "            - idx_to_token: Dictionary mapping indices back to original tokens.\n",
    "    \"\"\"\n",
    "    # Print a message to indicate the start of token mapping\n",
    "    print(\"Creating token mapping for efficient embedding...\")\n",
    "    \n",
    "    # Create a dictionary mapping string representations to their original token values\n",
    "    str_to_token = {}\n",
    "    for token in original_tokens:\n",
    "        token_str = str(token)  # Convert token to string for consistent keying\n",
    "        if token_str not in str_to_token:\n",
    "            str_to_token[token_str] = token  # Store the original token for each string\n",
    "    \n",
    "    # Create a dictionary mapping token strings to contiguous indices (0 to vocab_size-1)\n",
    "    token_to_idx = {token_str: idx for idx, token_str in enumerate(unique_strings)}\n",
    "    \n",
    "    # Create a dictionary mapping indices back to original tokens\n",
    "    idx_to_token = {idx: str_to_token[token_str] for idx, token_str in enumerate(unique_strings)}\n",
    "    \n",
    "    # Print confirmation of the number of unique tokens mapped\n",
    "    print(f\"  - Mapped {len(unique_strings)} unique array tokens to indices 0-{len(unique_strings)-1}\")\n",
    "    \n",
    "    # Return the bidirectional mappings for use in token processing\n",
    "    return token_to_idx, idx_to_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e63e3-530e-43b8-9880-8e6bcf8474bf",
   "metadata": {},
   "source": [
    "### Dataset Remapping to Contiguous Indices\n",
    "This section defines the remap_dataset function, which transforms the original MIDI token dataset for instrument 29 (electric guitar) into a tensor of contiguous integer indices using the token_to_idx mapping. This remapping is crucial for preparing the data for input to the melody transformer model, as it ensures that tokens are represented as sequential integers suitable for the model's embedding layer. The function converts each token to its string representation, maps it to the corresponding index, and creates a PyTorch tensor with the remapped indices. It also provides diagnostic output, including the total number of tokens remapped and the range of indices, to verify the transformation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6275e2e7-fc44-4812-a789-574eba1d32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_dataset(data_tokens, token_to_idx):\n",
    "    \"\"\"\n",
    "    Remaps the dataset to use contiguous indices instead of original token arrays.\n",
    "    \n",
    "    Args:\n",
    "        data_tokens: List or array of original tokens from the MIDI dataset.\n",
    "        token_to_idx: Dictionary mapping token strings to contiguous indices.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of remapped indices with dtype torch.long, suitable for model input.\n",
    "    \"\"\"\n",
    "    # Print a message to indicate the start of dataset remapping\n",
    "    print(\"Remapping dataset to use contiguous indices...\")\n",
    "    \n",
    "    # Initialize an empty list to store the remapped indices\n",
    "    remapped_indices = []\n",
    "    # Iterate through each token in the dataset\n",
    "    for token in data_tokens:\n",
    "        token_str = str(token)  # Convert the token to its string representation\n",
    "        # Append the corresponding index from the token_to_idx mapping\n",
    "        remapped_indices.append(token_to_idx[token_str])\n",
    "    \n",
    "    # Convert the list of indices to a PyTorch tensor with long integer type\n",
    "    remapped_data = torch.tensor(remapped_indices, dtype=torch.long)\n",
    "    \n",
    "    # Print diagnostic information about the remapping process\n",
    "    print(f\"  - Remapped {len(data_tokens)} array tokens\")\n",
    "    # Report the range of indices in the remapped dataset\n",
    "    print(f\"  - Remapped token range: {remapped_data.min().item()} to {remapped_data.max().item()}\")\n",
    "    \n",
    "    # Return the remapped dataset as a tensor\n",
    "    return remapped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b0145-6679-4655-90e4-ad01c262e5d3",
   "metadata": {},
   "source": [
    "### Vocabulary Analysis and Dataset Preparation\n",
    "This section orchestrates the preprocessing of the MIDI token dataset for instrument 29 (electric guitar) to prepare it for training the melody transformer model. It begins by creating a copy of the raw token data to preserve the original dataset. The analyze_vocabulary function is called to compute the vocabulary size and extract statistics, such as unique tokens and their frequencies, which inform the model's embedding layer design. The create_token_mapping function generates bidirectional mappings between tokens and contiguous indices, enabling efficient embedding. The dataset is then remapped to these indices using remap_dataset, converting the tokens into a PyTorch tensor suitable for model input. The remapped data is transferred to the selected device (GPU or CPU) for training, and a sample of the remapped data is printed for verification. Finally, the dynamic vocabulary size is reported, which is critical for configuring the embedding and output layers of the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f13b975d-96fd-466e-8838-3c60ad5e44eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing vocabulary from the dataset...\n",
      "Vocabulary Analysis Results:\n",
      "  - Total unique tokens (vocab size): 6201\n",
      "  - Total tokens in dataset: 7254\n",
      "  - Average token frequency: 1.17\n",
      "  - Top 10 most frequent tokens:\n",
      "    [53, 4.14, 4.45, 68]: 3 occurrences (0.04%)\n",
      "    [48, 4.14, 4.46, 70]: 3 occurrences (0.04%)\n",
      "    [41, 4.14, 4.46, 74]: 3 occurrences (0.04%)\n",
      "    [53, 4.53, 4.59, 64]: 3 occurrences (0.04%)\n",
      "    [48, 4.53, 4.6, 86]: 3 occurrences (0.04%)\n",
      "    [53, 4.66, 4.73, 64]: 3 occurrences (0.04%)\n",
      "    [48, 4.66, 4.73, 76]: 3 occurrences (0.04%)\n",
      "    [53, 4.91, 4.99, 62]: 3 occurrences (0.04%)\n",
      "    [47, 4.91, 5.0, 96]: 3 occurrences (0.04%)\n",
      "    [53, 5.17, 5.4, 74]: 3 occurrences (0.04%)\n",
      "  - Some rare tokens (bottom 5):\n",
      "    [82, 225.55, 225.7, 127]: 1 occurrences (0.01%)\n",
      "    [79, 225.74, 225.86, 113]: 1 occurrences (0.01%)\n",
      "    [82, 225.87, 225.96, 106]: 1 occurrences (0.01%)\n",
      "    [79, 225.99, 226.11, 119]: 1 occurrences (0.01%)\n",
      "    [84, 226.09, 226.74, 100]: 1 occurrences (0.01%)\n",
      "Creating token mapping for efficient embedding...\n",
      "  - Mapped 6201 unique array tokens to indices 0-6200\n",
      "Remapping dataset to use contiguous indices...\n",
      "  - Remapped 7254 array tokens\n",
      "  - Remapped token range: 0 to 6200\n",
      "Sample of remapped data: tensor([2799, 5415, 4089, 2762, 1425, 2774, 5874, 3563, 5418, 3356, 3970, 2014,\n",
      "        5076, 4779, 3487, 2951, 3717, 3904, 5972, 5026])...\n",
      "\n",
      "--- DYNAMIC VOCABULARY SIZE: 6201 ---\n",
      "This will be used for embedding and output layer dimensions.\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the raw tokens to avoid modifying the original data\n",
    "raw_train_tokens = tokens.copy()  # Ensures the original tokens list remains unchanged\n",
    "\n",
    "# Analyze the vocabulary to determine its size and statistics\n",
    "# Calls analyze_vocabulary to compute unique tokens, total tokens, and frequency statistics\n",
    "VOCAB_SIZE, vocab_stats = analyze_vocabulary(raw_train_tokens)\n",
    "\n",
    "# Create bidirectional mappings between token strings and contiguous indices\n",
    "# token_to_idx maps token strings to indices; idx_to_token maps indices back to original tokens\n",
    "token_to_idx, idx_to_token = create_token_mapping(vocab_stats['unique_strings'], raw_train_tokens)\n",
    "\n",
    "# Remap the dataset to use contiguous indices for efficient model input\n",
    "# Converts the raw tokens into a PyTorch tensor of indices\n",
    "train_data = remap_dataset(raw_train_tokens, token_to_idx)\n",
    "\n",
    "# Print a sample of the first 20 remapped indices for verification\n",
    "print(f\"Sample of remapped data: {train_data[:20]}...\")\n",
    "\n",
    "# Move the remapped dataset to the selected device (GPU or CPU) for training\n",
    "train_data = train_data.to(device)  # Ensures compatibility with the model's computation device\n",
    "\n",
    "# Print the dynamically determined vocabulary size, which will be used for model configuration\n",
    "print(f\"\\n--- DYNAMIC VOCABULARY SIZE: {VOCAB_SIZE} ---\")\n",
    "print(f\"This will be used for embedding and output layer dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48ff51-3e61-4b2d-b8aa-6ccbd3ad8fc1",
   "metadata": {},
   "source": [
    "### Batch Generation for Next Token Prediction\n",
    "This section defines the get_batch function, which generates batches of source and target sequences from the remapped MIDI dataset for training the melody transformer model on instrument 29 (electric guitar). The function is designed to support the next token prediction task, where the model learns to predict the subsequent token in a sequence given the preceding tokens. It randomly selects starting indices within the dataset, extracts sequences of a specified length (seq_length), and creates source (x) and target (y) tensors, where the target is the source sequence shifted one position to the right. A sample batch is generated and printed to illustrate the relationship between source and target sequences, aiding in the verification of the data preparation process for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85e99395-32b7-4269-b00c-1f8fab98686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's see an example of a single batch with batch_size=1 and seq_length=5:\n",
      "Source (x): [1152, 3694, 2462, 156, 5374]\n",
      "Target (y): [3694, 2462, 156, 5374, 2776]\n",
      "Notice that the target is the source sequence shifted one position to the right.\n"
     ]
    }
   ],
   "source": [
    "def get_batch(source_data, seq_length, batch_size):\n",
    "    \"\"\"\n",
    "    Generates a batch of source and target sequences for training.\n",
    "    This is the core of how we set up the \"next token prediction\" task.\n",
    "    \n",
    "    Args:\n",
    "        source_data: torch.Tensor, the remapped dataset of token indices.\n",
    "        seq_length: Integer, the length of each sequence (context window).\n",
    "        batch_size: Integer, the number of sequences in a batch.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (x, y)\n",
    "            - x: torch.Tensor, source sequences of shape (batch_size, seq_length).\n",
    "            - y: torch.Tensor, target sequences of shape (batch_size, seq_length).\n",
    "    \"\"\"\n",
    "    # Get the total length of the dataset\n",
    "    num_tokens = len(source_data)\n",
    "    \n",
    "    # Generate random starting indices for sequences\n",
    "    # Ensure there's enough room for seq_length + 1 to include the target token\n",
    "    start_indices = torch.randint(0, num_tokens - seq_length - 1, (batch_size,))\n",
    "    \n",
    "    # Create source sequences by stacking slices of source_data\n",
    "    # Each slice is of length seq_length, starting at a random index\n",
    "    x = torch.stack([source_data[i : i + seq_length] for i in start_indices])\n",
    "    \n",
    "    # Create target sequences, shifted one position to the right\n",
    "    # For each input token, the target is the next token in the sequence\n",
    "    y = torch.stack([source_data[i + 1 : i + seq_length + 1] for i in start_indices])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Demonstrate an example batch for clarity\n",
    "print(f\"\\nLet's see an example of a single batch with batch_size=1 and seq_length=5:\")\n",
    "# Generate a sample batch with batch_size=1 and seq_length=5\n",
    "x_sample, y_sample = get_batch(train_data, 5, 1)\n",
    "# Convert tensors to lists for readable output, squeezing to remove batch dimension\n",
    "print(f\"Source (x): {x_sample.squeeze().tolist()}\")\n",
    "print(f\"Target (y): {y_sample.squeeze().tolist()}\")\n",
    "# Highlight the relationship between source and target sequences\n",
    "print(\"Notice that the target is the source sequence shifted one position to the right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ee682-7a50-4748-b52f-7b4bbf82186f",
   "metadata": {},
   "source": [
    "### Positional Encoding for Transformer Input\n",
    "This section defines the PositionalEncoding class, a critical component of the melody transformer model designed for MIDI data from instrument 29 (electric guitar). The class implements positional encodings to inject information about the token positions in the sequence, as the transformer architecture lacks an inherent sense of order. It follows the standard sinusoidal encoding scheme proposed by Vaswani et al. (2017), where sine and cosine functions are used to create position-specific vectors that are added to the token embeddings. The class supports dropout for regularization and is designed to handle sequences up to a maximum length (max_len). The positional encodings are stored as a buffer, ensuring they are part of the model’s state but not trainable parameters. This implementation ensures that the model can capture the sequential nature of musical tokens during training and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a097b73-5f05-4b23-af81-9431ba5d5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects position information into the token embeddings.\n",
    "    Since the Transformer architecture itself doesn't have a notion of order,\n",
    "    we add these positional encodings to the input embeddings.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Integer, the dimensionality of the token embeddings.\n",
    "        dropout: Float, the dropout probability for regularization (default: 0.1).\n",
    "        max_len: Integer, the maximum sequence length to precompute encodings for (default: 5000).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()  # Initialize the parent nn.Module class\n",
    "        self.dropout = nn.Dropout(p=dropout)  # Initialize dropout layer with specified probability\n",
    "\n",
    "        # Create a matrix for positional encodings of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)  # Initialize zero matrix for positional encodings\n",
    "        \n",
    "        # Create a position tensor [0, 1, 2, ..., max_len-1] with shape (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Calculate the division term for the sine and cosine functions\n",
    "        # Uses the formula: exp(2i * -log(10000) / d_model) for even indices\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices (2i) in the positional encoding matrix\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices (2i+1) in the positional encoding matrix\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension and transpose to shape (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        # Register 'pe' as a buffer to include it in the model's state without training\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "        # Print confirmation of successful initialization\n",
    "        print(\"Initialized PositionalEncoding module.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to add positional encodings to input embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim], input token embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Input embeddings with positional encodings added, after applying dropout.\n",
    "        \"\"\"\n",
    "        # Add positional encodings to the input tensor, matching the sequence length\n",
    "        x = x + self.pe[:x.size(0), :]  # Slice pe to match input sequence length\n",
    "        \n",
    "        # Apply dropout to the combined embeddings for regularization\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3d862-cc87-48cc-a5b5-4c1aac81edda",
   "metadata": {},
   "source": [
    "### Transformer Model Architecture for Next Token Prediction\n",
    "This section defines the TransformerModel class, which implements the core architecture of the melody transformer for next token prediction on MIDI data for instrument 29 (electric guitar). The model, built using PyTorch’s nn.Module, integrates several key components: (1) a token embedding layer to map input token indices to dense vectors, (2) a positional encoding layer to incorporate sequence order, (3) a stack of transformer encoder layers for modeling complex dependencies in the musical sequences, and (4) a final linear layer to produce logits over the vocabulary for next token prediction. The model is initialized with hyperparameters such as vocabulary size, embedding dimension, number of attention heads, feed-forward dimension, number of encoder layers, and dropout rate. Weight initialization is performed to ensure stable training, and the forward pass includes detailed logging of tensor shapes for debugging and verification. This architecture is designed to capture the sequential and contextual patterns in MIDI token sequences, enabling the generation of coherent musical output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e086da2-4eb1-4720-97cd-78b5435a694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer model for sequence-to-sequence tasks.\n",
    "    In our case, it's used for next-token prediction.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Integer, the size of the token vocabulary.\n",
    "        d_model: Integer, the dimensionality of the token embeddings.\n",
    "        nhead: Integer, the number of attention heads in the multi-head attention mechanism.\n",
    "        d_hid: Integer, the dimension of the feed-forward network in each transformer layer.\n",
    "        nlayers: Integer, the number of transformer encoder layers.\n",
    "        dropout: Float, the dropout probability for regularization (default: 0.1).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, nhead, d_hid, nlayers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()  # Initialize the parent nn.Module class\n",
    "        self.model_type = 'Transformer'  # Identifier for the model type\n",
    "        self.d_model = d_model  # Store embedding dimension for use in forward pass\n",
    "        self.vocab_size = vocab_size  # Store vocabulary size for embedding and output layers\n",
    "\n",
    "        # 1. Token Embedding Layer: Maps input token indices to dense vectors\n",
    "        self.encoder = nn.Embedding(vocab_size, d_model)  # Embedding layer for tokens\n",
    "        print(f\"Initialized nn.Embedding: maps {vocab_size} tokens to {d_model}-dim vectors.\")\n",
    "\n",
    "        # 2. Positional Encoding: Adds positional information to token embeddings\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)  # Positional encoding layer\n",
    "\n",
    "        # 3. Transformer Encoder Layers: Core of the model for sequence modeling\n",
    "        # Define a single transformer encoder layer with specified parameters\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=False)\n",
    "        # Stack multiple encoder layers to form the transformer encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        print(f\"Initialized nn.TransformerEncoder with {nlayers} layers.\")\n",
    "\n",
    "        # 4. Final Linear Layer (Decoder): Maps transformer output to vocabulary logits\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)  # Linear layer for output logits\n",
    "        print(f\"Initialized final nn.Linear decoder: maps {d_model}-dim vectors to {vocab_size} (vocab size) logits.\")\n",
    "\n",
    "        # Initialize weights for the embedding and linear layers\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initializes weights for the embedding and linear layers.\"\"\"\n",
    "        initrange = 0.1  # Define the range for uniform weight initialization\n",
    "        # Initialize embedding layer weights with uniform distribution\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # Zero the bias of the decoder layer\n",
    "        self.decoder.bias.data.zero_()\n",
    "        # Initialize decoder layer weights with uniform distribution\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size], input token indices.\n",
    "            src_mask: Tensor, the mask for the src sequence to prevent attention to padding tokens.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output logits of shape [seq_len, batch_size, vocab_size].\n",
    "        \"\"\"\n",
    "        # Log the start of the forward pass for debugging\n",
    "        print(\"\\n--- Inside Model Forward Pass ---\")\n",
    "        print(f\"Input `src` shape: {src.shape} [Sequence Length, Batch Size]\")\n",
    "\n",
    "        # 1. Embed the tokens and scale by sqrt(d_model) to stabilize gradients\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        print(f\"Shape after Embedding and Scaling: {src.shape} [Seq Len, Batch, Embedding Dim]\")\n",
    "\n",
    "        # 2. Add positional encoding to incorporate sequence order\n",
    "        src = self.pos_encoder(src)\n",
    "        print(f\"Shape after Positional Encoding: {src.shape} [Seq Len, Batch, Embedding Dim]\")\n",
    "\n",
    "        # 3. Pass through the transformer encoder layers to model sequence dependencies\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        print(f\"Shape after Transformer Encoder: {output.shape} [Seq Len, Batch, Embedding Dim]\")\n",
    "\n",
    "        # 4. Decode the output to produce logits over the vocabulary\n",
    "        output = self.decoder(output)\n",
    "        print(f\"Shape after Final Decoder Layer: {output.shape} [Seq Len, Batch, Vocab Size]\")\n",
    "        print(\"--- End of Model Forward Pass ---\\n\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e28be-dae3-494a-a415-081931c0ed1b",
   "metadata": {},
   "source": [
    "### Causal Mask Generation for Transformer Training\n",
    "This section defines the generate_square_subsequent_mask function, which creates a square causal mask for the melody transformer model used with MIDI data for instrument 29 (electric guitar). The mask ensures that during training, the model only attends to previous and current tokens in the sequence, preventing it from accessing future tokens, which is critical for the next token prediction task. The function generates a square matrix of size sz (sequence length), where the upper triangular portion (excluding the diagonal) is filled with negative infinity to mask future tokens, and the lower triangular portion (including the diagonal) is filled with zeros to allow attention. This causal masking enforces the autoregressive property of the transformer, ensuring it learns to predict the next token based solely on prior context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87b1b1d9-6e99-4568-9714-4bfe11dbbcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"\n",
    "    Generates a square causal mask for the sequence.\n",
    "    The masked positions are filled with -inf.\n",
    "    Unmasked positions are 0. This prevents the model from \"cheating\" by\n",
    "    looking at future tokens during training.\n",
    "    \n",
    "    Args:\n",
    "        sz: Integer, the size of the square mask (sequence length).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A square causal mask of shape [sz, sz], where unmasked positions\n",
    "                     are 0.0 and masked positions are -inf.\n",
    "    \"\"\"\n",
    "    # Create an upper triangular matrix of ones (True where position i <= j)\n",
    "    # triu returns the upper triangle of a matrix, with 1s on and above the diagonal\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    \n",
    "    # Convert the boolean mask to a float tensor\n",
    "    # Fill masked positions (False, i.e., future tokens) with -inf\n",
    "    # Fill unmasked positions (True, i.e., current and past tokens) with 0.0\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    \n",
    "    return mask  # Return the causal mask tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfee8cf-6867-4c58-891d-ba31cfbce706",
   "metadata": {},
   "source": [
    "### Model Instantiation and Training Setup\n",
    "This section initializes the melody transformer model for generating musical sequences from MIDI data for instrument 29 (electric guitar). The TransformerModel is instantiated using the dynamically calculated vocabulary size (VOCAB_SIZE) and predefined hyperparameters for embedding dimension, number of attention heads, feed-forward dimension, number of encoder layers, and dropout rate. The model is moved to the selected device (GPU or CPU) for computation. The loss function is defined as cross-entropy loss, suitable for the next token prediction task, and the Adam optimizer is configured with the specified learning rate. Diagnostic outputs confirm the model’s initialization and report the total number of trainable parameters, providing insight into the model’s complexity for the academic context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "104990e2-a63d-460c-85da-d897d092a889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized nn.Embedding: maps 6201 tokens to 64-dim vectors.\n",
      "Initialized PositionalEncoding module.\n",
      "Initialized nn.TransformerEncoder with 2 layers.\n",
      "Initialized final nn.Linear decoder: maps 64-dim vectors to 6201 (vocab size) logits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saahil/projects/MelodyGenerator/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with dynamic vocabulary size: 6201\n",
      "Total model parameters: 899,897\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the transformer model with the specified hyperparameters\n",
    "# VOCAB_SIZE is dynamically determined from the dataset\n",
    "# Other parameters (EMBEDDING_DIM, NUM_HEADS, FF_DIM, NUM_ENCODER_LAYERS, DROPOUT) are predefined\n",
    "model = TransformerModel(\n",
    "    VOCAB_SIZE,          # Vocabulary size from dataset analysis\n",
    "    EMBEDDING_DIM,       # Dimensionality of token embeddings\n",
    "    NUM_HEADS,           # Number of attention heads in multi-head attention\n",
    "    FF_DIM,              # Dimension of feed-forward network in transformer layers\n",
    "    NUM_ENCODER_LAYERS,  # Number of transformer encoder layers\n",
    "    DROPOUT              # Dropout probability for regularization\n",
    ").to(device)  # Move the model to the selected device (GPU or CPU)\n",
    "\n",
    "# Define the loss function for next token prediction\n",
    "# CrossEntropyLoss combines log softmax and negative log likelihood loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the Adam optimizer with the specified learning rate\n",
    "# Optimizes all trainable parameters of the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Print confirmation of model initialization with the dynamic vocabulary size\n",
    "print(f\"Model initialized with dynamic vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Calculate and print the total number of trainable parameters in the model\n",
    "# Summing the number of elements (numel) in each parameter tensor\n",
    "print(f\"Total model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da63273-34be-4de9-bc1d-cc37a57fb2c5",
   "metadata": {},
   "source": [
    "### Training Loop for the Melody Transformer\n",
    "This section implements the train function, which defines the training loop for one epoch of the melody transformer model, designed for next token prediction on MIDI data for instrument 29 (electric guitar). The function sets the model to training mode, generates a causal mask to enforce autoregressive behavior, and iterates over the dataset in batches. For each batch, it retrieves source and target sequences, permutes them to match the transformer’s expected input shape, computes the model’s output, and calculates the cross-entropy loss. Gradients are computed, clipped to prevent exploding gradients, and used to update the model weights via the Adam optimizer. Training progress is logged periodically, including loss, perplexity, learning rate, and batch processing time, to monitor convergence and performance. The loop runs for the specified number of epochs, with timing information printed for each epoch. This implementation ensures robust training while providing detailed diagnostics for academic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe9c5607-3d18-46cf-8035-3f43361c6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "epoch_perplexities = []\n",
    "epoch_grad_norms = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    total_grad_norm = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(SEQUENCE_LENGTH).to(device)\n",
    "    num_batches = len(train_data) // (SEQUENCE_LENGTH * BATCH_SIZE)\n",
    "    batch_losses = []\n",
    "\n",
    "    print(f\"\\n--- Starting Epoch {epoch} ---\")\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1 - SEQUENCE_LENGTH, SEQUENCE_LENGTH)):\n",
    "        data, targets = get_batch(train_data, SEQUENCE_LENGTH, BATCH_SIZE)\n",
    "        data = data.permute(1, 0).to(device)\n",
    "        targets = targets.permute(1, 0).to(device)\n",
    "\n",
    "        if batch == 0:\n",
    "            print(f\"Shape of data batch: {data.shape}\")\n",
    "            print(f\"Shape of target batch: {targets.shape}\")\n",
    "            print(f\"Shape of causal mask: {src_mask.shape}\")\n",
    "            print(\"Starting batch iterations...\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if batch == 0 and epoch == 1:\n",
    "            output = model(data, src_mask)\n",
    "        else:\n",
    "            _print = __builtins__.print\n",
    "            __builtins__.print = lambda *args, **kwargs: None\n",
    "            output = model(data, src_mask)\n",
    "            __builtins__.print = _print\n",
    "\n",
    "        loss = criterion(output.view(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_grad_norm += grad_norm.item()\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / LOG_INTERVAL\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            cur_ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches * BATCH_SIZE:5d} batches | '\n",
    "                  f'lr {lr:02.5f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {cur_ppl:8.2f} | grad norm {grad_norm:5.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "    # Epoch summary\n",
    "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
    "    avg_ppl = math.exp(avg_loss)\n",
    "    avg_grad_norm = total_grad_norm / len(batch_losses)\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f'--- End of Epoch {epoch} | Avg Loss: {avg_loss:.2f} | '\n",
    "          f'Avg Perplexity: {avg_ppl:.2f} | Avg Grad Norm: {avg_grad_norm:.2f} | '\n",
    "          f'Time: {epoch_time:.2f}s ---')\n",
    "\n",
    "    # Store metrics\n",
    "    epoch_losses.append(avg_loss)\n",
    "    epoch_perplexities.append(avg_ppl)\n",
    "    epoch_grad_norms.append(avg_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c860664-05d0-4c5d-92b1-fd34538b4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics():\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    epochs = list(range(1, len(epoch_losses) + 1))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=epochs, y=epoch_losses, marker='o', label='Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig('graphs/loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot perplexity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=epochs, y=epoch_perplexities, marker='o', label='Perplexity')\n",
    "    plt.title('Training Perplexity Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.savefig('graphs/perplexity_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot gradient norm\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=epochs, y=epoch_grad_norms, marker='o', label='Gradient Norm')\n",
    "    plt.title('Gradient Norm Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.savefig('graphs/grad_norm_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dac96ba-f653-4e8e-87c7-69de355897e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Epoch 1 ---\n",
      "Shape of data batch: torch.Size([64, 32])\n",
      "Shape of target batch: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "\n",
      "--- Inside Model Forward Pass ---\n",
      "Input `src` shape: torch.Size([64, 32]) [Sequence Length, Batch Size]\n",
      "Shape after Embedding and Scaling: torch.Size([64, 32, 64]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Positional Encoding: torch.Size([64, 32, 64]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Transformer Encoder: torch.Size([64, 32, 64]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Final Decoder Layer: torch.Size([64, 32, 6201]) [Seq Len, Batch, Vocab Size]\n",
      "--- End of Model Forward Pass ---\n",
      "\n",
      "--- End of Epoch 1 | Avg Loss: 6.95 | Avg Perplexity: 1042.40 | Avg Grad Norm: 0.40 | Time: 0.97s ---\n",
      "\n",
      "--- Starting Epoch 2 ---\n",
      "Shape of data batch: torch.Size([64, 32])\n",
      "Shape of target batch: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "--- End of Epoch 2 | Avg Loss: 3.28 | Avg Perplexity: 26.52 | Avg Grad Norm: 0.57 | Time: 0.62s ---\n",
      "\n",
      "--- Starting Epoch 3 ---\n",
      "Shape of data batch: torch.Size([64, 32])\n",
      "Shape of target batch: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "--- End of Epoch 3 | Avg Loss: 0.93 | Avg Perplexity: 2.52 | Avg Grad Norm: 0.39 | Time: 0.62s ---\n",
      "\n",
      "--- Starting Epoch 4 ---\n",
      "Shape of data batch: torch.Size([64, 32])\n",
      "Shape of target batch: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "--- End of Epoch 4 | Avg Loss: 0.26 | Avg Perplexity: 1.30 | Avg Grad Norm: 0.16 | Time: 0.61s ---\n",
      "\n",
      "--- Starting Epoch 5 ---\n",
      "Shape of data batch: torch.Size([64, 32])\n",
      "Shape of target batch: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "--- End of Epoch 5 | Avg Loss: 0.14 | Avg Perplexity: 1.15 | Avg Grad Norm: 0.10 | Time: 0.61s ---\n",
      "Training completed. Plots saved in 'graphs' folder.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "plot_metrics()\n",
    "print(\"Training completed. Plots saved in 'graphs' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ee41a-e2d4-4e1b-b292-d681e599dcdb",
   "metadata": {},
   "source": [
    "### Sequence Generation with the Melody Transformer\n",
    "This section defines the predict function, which generates a musical sequence token by token using the trained melody transformer model for MIDI data corresponding to instrument 29 (electric guitar). The function takes a seed sequence of token indices, generates a sequence up to a specified maximum length (max_len), and optionally converts the output indices back to their original token representations using the idx_to_token mapping. The model is set to evaluation mode to disable dropout, and a causal mask ensures that predictions are autoregressive, attending only to previous tokens. The function applies softmax to the model’s output logits to obtain probabilities, samples the next token using multinomial sampling, and iteratively builds the sequence. This approach enables the generation of coherent musical sequences, which is critical for evaluating the model’s performance in a creative and academic context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04981baf-08fe-462c-bd8d-2f5257452afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, seed_sequence, max_len=50, idx_to_token=None):\n",
    "    \"\"\"\n",
    "    Generates a sequence token by token based on a seed.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained TransformerModel instance.\n",
    "        seed_sequence: List of integers, the initial sequence of token indices.\n",
    "        max_len: Integer, the maximum number of tokens to generate (default: 50).\n",
    "        idx_to_token: Dictionary, mapping indices to original tokens (optional).\n",
    "    \n",
    "    Returns:\n",
    "        List: The generated sequence of token indices.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode, disabling dropout\n",
    "    # Print the input seed sequence for verification\n",
    "    print(f\"Seed sequence (indices): {seed_sequence}\")\n",
    "    # If idx_to_token is provided, convert and print the seed in original token format\n",
    "    if idx_to_token:\n",
    "        original_tokens = [idx_to_token[idx] for idx in seed_sequence]\n",
    "        print(f\"Seed sequence (original array tokens): {original_tokens}\")\n",
    "    \n",
    "    # Convert the seed sequence to a tensor with shape [seq_len, batch_size=1]\n",
    "    input_tensor = torch.tensor(seed_sequence, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    \n",
    "    # Initialize the generated sequence with a copy of the seed\n",
    "    generated_sequence = seed_sequence.copy()\n",
    "\n",
    "    # Disable gradient computation for inference to save memory and computation\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_len):  # Generate up to max_len tokens\n",
    "            # Get the current sequence length for creating the causal mask\n",
    "            current_seq_len = input_tensor.size(0)\n",
    "            # Generate a causal mask to prevent attending to future tokens\n",
    "            mask = generate_square_subsequent_mask(current_seq_len).to(device)\n",
    "            \n",
    "            # Run the model’s forward pass to get output logits\n",
    "            if step == 0:\n",
    "                output = model(input_tensor, mask)  # Include logging for the first step\n",
    "            else:\n",
    "                # Suppress print statements in the forward pass for cleaner logs\n",
    "                _print = __builtins__.print\n",
    "                __builtins__.print = lambda *args, **kwargs: None\n",
    "                output = model(input_tensor, mask)\n",
    "                __builtins__.print = _print\n",
    "            \n",
    "            # Extract logits for the last token in the sequence\n",
    "            last_token_logits = output[-1, 0, :]  # Shape: [vocab_size]\n",
    "            \n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample the next token index from the probability distribution\n",
    "            next_token = torch.multinomial(probabilities, 1).item()\n",
    "            \n",
    "            # Convert the predicted index to the original token if idx_to_token is provided\n",
    "            predicted_original = idx_to_token[next_token] if idx_to_token else next_token\n",
    "            \n",
    "            # Append the predicted token index to the generated sequence\n",
    "            generated_sequence.append(next_token)\n",
    "            \n",
    "            # Update the input tensor by appending the new token\n",
    "            # Shape becomes [current_seq_len + 1, 1]\n",
    "            input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=device)], dim=0)\n",
    "\n",
    "    # Return the complete generated sequence of token indices\n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fece59e-0381-4d23-9c56-b97e07e19e74",
   "metadata": {},
   "source": [
    "### Sequence Generation and Output Display\n",
    "This section demonstrates the application of the predict function to generate a musical sequence for instrument 29 (electric guitar) using a predefined seed sequence of token indices. The seed sequence, represented as remapped indices, is passed to the predict function along with the idx_to_token mapping to generate a sequence of up to 100 tokens. The generated sequence is then printed in both its remapped index form and its original token form, providing a clear view of the model’s output in the context of the MIDI dataset. This step is crucial for evaluating the melody transformer’s ability to produce coherent musical sequences, and the output can be analyzed for musical quality or further processed into MIDI format for playback in an academic setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "779c9bf4-cb7f-4d41-9b6d-a639f7f07111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed sequence (indices): [3456, 2345, 1234, 2333]\n",
      "Seed sequence (original array tokens): [[41, 12.89, 13.07, 117], [46, 53.56, 53.72, 105], [45, 10.22, 10.55, 127], [57, 162.85, 163.37, 127]]\n",
      "\n",
      "--- Inside Model Forward Pass ---\n",
      "Input `src` shape: torch.Size([4, 1]) [Sequence Length, Batch Size]\n",
      "Shape after Embedding and Scaling: torch.Size([4, 1, 64]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Positional Encoding: torch.Size([4, 1, 64]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Transformer Encoder: torch.Size([4, 1, 64]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Final Decoder Layer: torch.Size([4, 1, 6201]) [Seq Len, Batch, Vocab Size]\n",
      "--- End of Model Forward Pass ---\n",
      "\n",
      "Original Seed (indices): [3456, 2345, 1234, 2333]\n",
      "Original Seed (original array tokens): [[41, 12.89, 13.07, 117], [46, 53.56, 53.72, 105], [45, 10.22, 10.55, 127], [57, 162.85, 163.37, 127]]\n",
      "Generated Sequence (indices): [3456, 2345, 1234, 2333, 3136, 47, 5711, 1302, 4033, 3134, 3295, 649, 1627, 3456, 4323, 1657, 5779, 5392, 5214, 4501, 3915, 2913, 2296, 223, 747, 4505, 4287, 5974, 2469, 637, 1213, 4472, 4959, 2115, 5364, 1695, 327, 5436, 6088, 3357, 3077, 2818, 967, 5815, 3956, 4811, 2000, 221, 2986, 1150, 1283, 4787, 535, 3618, 3199, 4726, 2336, 1705, 203, 3507, 5925, 2477, 5620, 2154, 1006, 4588, 3504, 1674, 388, 4900, 5140, 3402, 5935, 3139, 3125, 4453, 5625, 323, 3728, 323, 436, 842, 5446, 1889, 3828, 4597, 3951, 3362, 2277, 422, 1829, 5137, 3083, 1955, 5272, 706, 5534, 4885, 1126, 1811, 4700, 543, 6014, 6184]\n",
      "Generated Sequence (original array tokens): [[41, 12.89, 13.07, 117], [46, 53.56, 53.72, 105], [45, 10.22, 10.55, 127], [57, 162.85, 163.37, 127], [57, 163.49, 163.68, 127], [48, 11.9, 12.07, 123], [41, 12.23, 12.31, 99], [48, 12.23, 12.33, 113], [41, 12.4, 12.57, 117], [50, 12.4, 12.59, 123], [41, 12.73, 12.82, 102], [48, 12.73, 12.82, 113], [48, 12.89, 13.07, 123], [41, 12.89, 13.07, 117], [69, 325.98, 327.24, 89], [48, 327.86, 328.59, 98], [45, 327.86, 328.61, 98], [64, 327.86, 328.62, 98], [60, 327.86, 328.62, 98], [52, 328.61, 328.66, 98], [45, 328.62, 328.67, 98], [57, 328.62, 328.71, 98], [50, 328.98, 330.11, 97], [62, 328.98, 330.11, 98], [57, 330.11, 330.16, 98], [62, 330.11, 330.16, 98], [45, 328.98, 330.26, 97], [57, 330.86, 331.61, 97], [64, 330.88, 331.63, 112], [60, 330.88, 331.64, 98], [60, 331.61, 331.64, 98], [57, 331.61, 331.66, 98], [52, 331.61, 331.66, 98], [45, 330.86, 331.74, 97], [57, 331.98, 333.1, 89], [52, 331.98, 333.11, 89], [61, 331.98, 333.11, 96], [52, 333.11, 333.16, 98], [61, 333.11, 333.16, 98], [57, 333.11, 333.16, 98], [45, 331.98, 333.24, 89], [69, 331.98, 333.24, 89], [48, 333.86, 334.59, 98], [45, 333.86, 334.61, 98], [64, 333.86, 334.62, 98], [60, 333.86, 334.62, 98], [52, 334.61, 334.66, 98], [45, 334.62, 334.67, 98], [57, 334.62, 334.71, 98], [50, 334.98, 336.11, 97], [62, 334.98, 336.11, 98], [57, 336.11, 336.16, 98], [62, 336.11, 336.16, 98], [45, 334.98, 336.26, 97], [57, 336.86, 337.61, 97], [64, 336.88, 337.63, 112], [60, 336.88, 337.64, 98], [60, 337.61, 337.64, 98], [57, 337.61, 337.66, 98], [52, 337.61, 337.66, 98], [45, 336.86, 337.74, 97], [57, 337.98, 339.1, 89], [52, 337.98, 339.11, 89], [61, 337.98, 339.11, 96], [52, 339.11, 339.16, 98], [61, 339.11, 339.16, 98], [57, 339.11, 339.16, 98], [45, 337.98, 339.24, 89], [69, 337.98, 339.24, 89], [48, 339.86, 340.59, 98], [45, 339.86, 340.61, 98], [64, 339.86, 340.62, 98], [60, 339.86, 340.62, 98], [43, 82.81, 82.93, 122], [52, 82.81, 82.98, 124], [36, 351.86, 352.98, 100], [66, 352.98, 354.11, 97], [57, 352.98, 354.11, 97], [62, 352.98, 354.11, 97], [57, 352.98, 354.11, 97], [57, 354.11, 354.16, 98], [62, 354.11, 354.16, 98], [45, 352.98, 354.26, 97], [57, 354.86, 355.61, 97], [48, 227.0, 227.37, 84], [53, 235.0, 235.31, 110], [41, 235.02, 235.35, 74], [48, 235.01, 235.37, 92], [53, 242.98, 243.33, 106], [41, 242.99, 243.36, 70], [48, 242.98, 243.37, 84], [53, 250.9, 251.22, 106], [41, 250.91, 251.24, 70], [48, 250.9, 251.26, 76], [41, 251.6, 253.88, 84], [48, 251.6, 253.94, 86], [53, 251.6, 254.03, 92], [48, 253.99, 254.05, 28], [53, 65.29, 65.41, 95], [46, 65.29, 65.42, 104], [48, 65.46, 65.75, 115], [53, 65.46, 65.77, 115], [41, 65.46, 65.79, 104], [48, 67.77, 68.01, 127]]\n"
     ]
    }
   ],
   "source": [
    "# Define a seed sequence of token indices in the remapped space\n",
    "# These indices correspond to tokens in the vocabulary for instrument 29 (electric guitar)\n",
    "seed_indices = [3456, 2345, 1234, 2333]  # Example indices for the seed sequence\n",
    "\n",
    "# Generate a sequence using the predict function\n",
    "# max_len=100 specifies the maximum length of the generated sequence\n",
    "# idx_to_token is used to map indices back to original tokens\n",
    "predicted_sequence = predict(model, seed_indices, max_len=100, idx_to_token=idx_to_token)\n",
    "\n",
    "# Print the original seed sequence in remapped indices\n",
    "print(f\"Original Seed (indices): {seed_indices}\")\n",
    "\n",
    "# Convert and print the seed sequence to original token representations\n",
    "print(f\"Original Seed (original array tokens): {[idx_to_token[idx] for idx in seed_indices]}\")\n",
    "\n",
    "# Print the full generated sequence in remapped indices\n",
    "print(f\"Generated Sequence (indices): {predicted_sequence}\")\n",
    "\n",
    "# Convert and print the generated sequence to original token representations\n",
    "print(f\"Generated Sequence (original array tokens): {[idx_to_token[idx] for idx in predicted_sequence]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1ca14-7e34-4612-856a-4f30d370d35f",
   "metadata": {},
   "source": [
    "### Saving Generated Sequence to JSON File\n",
    "This section handles the storage of the generated musical sequence for instrument 29 (electric guitar) as a JSON file. The code creates a directory named generated_data if it does not already exist, ensuring robust file handling across different systems using the pathlib library. The generated sequence, represented as remapped indices in predicted_sequence, is converted back to its original token representations using the idx_to_token mapping. These tokens are then saved to a JSON file (29.json) in the specified directory. This step is critical for preserving the model’s output for further analysis, such as MIDI file generation or evaluation in an academic context, enabling reproducibility and documentation of the melody transformer’s results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30561a5f-cda8-453d-a86f-0ee495f12af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory for generated data\n",
    "folder = Path(\"generated_data\")\n",
    "# Create the directory, including parent directories if needed, and ignore if it already exists\n",
    "folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the file path for the output JSON file\n",
    "file_path = folder / \"29.json\"\n",
    "\n",
    "# Open the file in write mode and save the generated sequence\n",
    "# Convert the sequence indices back to original tokens using idx_to_token\n",
    "with file_path.open(\"w\") as f:\n",
    "    json.dump([idx_to_token[idx] for idx in predicted_sequence], f)  # Save tokens as a JSON list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2d010-81ad-4da0-8cc2-f56ce14fba74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
