{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1237f52-e169-416e-b95b-1256516f0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc41062d-4df4-4b1c-b478-6dfba89bd358",
   "metadata": {},
   "source": [
    "## Set the device to CUDA (GPU) if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e64d11-8d3b-496c-be15-9684d890891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5450e4a-dd1f-4655-a717-815faac1901e",
   "metadata": {},
   "source": [
    "## Parameters used for data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c5a38e2-858a-41e1-946b-aee4ffe0a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum possible token value for random generation\n",
    "MAX_TOKEN_VALUE = 100\n",
    "# Size of the dataset to generate\n",
    "DATASET_SIZE = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9051ce-f308-498a-b9a0-a49a6bb6d61f",
   "metadata": {},
   "source": [
    "## Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7df23c9-e16c-42a0-b3d3-59f0ea3aad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensionality of the token embeddings. This is the size of the vector\n",
    "# that will represent each token.\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "# The number of attention heads in the multi-head attention mechanism.\n",
    "# The embedding dimension must be divisible by this number.\n",
    "NUM_HEADS = 8\n",
    "\n",
    "# The number of Transformer encoder layers to stack.\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "\n",
    "# The dimension of the feed-forward network within each Transformer layer.\n",
    "FF_DIM = 1024\n",
    "\n",
    "# The dropout probability to be applied in the model.\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8cb3e-599f-40ff-acb2-26b29e366987",
   "metadata": {},
   "source": [
    "## Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5130a560-733a-43c1-ba5d-45633d4a96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of independent sequences to process in parallel.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# The length of the subsequences to be used for training. This is also known\n",
    "# as the context window or \"backpropagation through time\" (BPTT) length.\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "# The number of epochs to train the model for.\n",
    "EPOCHS = 5\n",
    "\n",
    "# The learning rate for the optimizer.\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# How often to log training progress (in batches).\n",
    "LOG_INTERVAL = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f974ae-6012-487e-b4ab-4e4245a5b7ce",
   "metadata": {},
   "source": [
    "## Generate random sequence for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a0870f2-9890-407b-8aee-1ce28925409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sequence(length, max_token_value):\n",
    "    \"\"\"Generates a long sequence of random discrete tokens.\"\"\"\n",
    "    # Create a tensor of random integers between 0 and max_token_value\n",
    "    # Note: We use a non-uniform distribution to make it more realistic\n",
    "    # Some tokens will be more frequent than others\n",
    "    tokens = []\n",
    "    for _ in range(length):\n",
    "        # Use a power law distribution to make some tokens more frequent\n",
    "        # This simulates real-world token distributions (Zipf's law)\n",
    "        if np.random.random() < 0.7:  # 70% chance for common tokens\n",
    "            token = np.random.randint(0, max_token_value // 4)\n",
    "        elif np.random.random() < 0.9:  # 20% chance for medium tokens\n",
    "            token = np.random.randint(max_token_value // 4, max_token_value // 2)\n",
    "        else:  # 10% chance for rare tokens\n",
    "            token = np.random.randint(max_token_value // 2, max_token_value)\n",
    "        tokens.append(token)\n",
    "    \n",
    "    return torch.tensor(tokens, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb2461-c325-49a3-8ea3-7182b8247b29",
   "metadata": {},
   "source": [
    "## Analyze Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f544f11-a4bd-4746-b943-291fadb299a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_vocabulary(data):\n",
    "    \"\"\"\n",
    "    Analyzes the dataset to determine vocabulary characteristics.\n",
    "    Returns vocabulary size and other statistics.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing vocabulary from the dataset...\")\n",
    "    \n",
    "    # Get unique tokens and their frequencies\n",
    "    unique_tokens, counts = torch.unique(data, return_counts=True)\n",
    "    vocab_size = len(unique_tokens)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    min_token = unique_tokens.min().item()\n",
    "    max_token = unique_tokens.max().item()\n",
    "    total_tokens = len(data)\n",
    "    \n",
    "    # Sort by frequency for analysis\n",
    "    sorted_indices = torch.argsort(counts, descending=True)\n",
    "    sorted_tokens = unique_tokens[sorted_indices]\n",
    "    sorted_counts = counts[sorted_indices]\n",
    "    \n",
    "    print(f\"Vocabulary Analysis Results:\")\n",
    "    print(f\"  - Total unique tokens (vocab size): {vocab_size}\")\n",
    "    print(f\"  - Token range: {min_token} to {max_token}\")\n",
    "    print(f\"  - Total tokens in dataset: {total_tokens}\")\n",
    "    print(f\"  - Average token frequency: {total_tokens / vocab_size:.2f}\")\n",
    "    \n",
    "    # Show top 10 most frequent tokens\n",
    "    print(f\"  - Top 10 most frequent tokens:\")\n",
    "    for i in range(min(10, len(sorted_tokens))):\n",
    "        token = sorted_tokens[i].item()\n",
    "        count = sorted_counts[i].item()\n",
    "        percentage = (count / total_tokens) * 100\n",
    "        print(f\"    Token {token}: {count} occurrences ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Show some rare tokens\n",
    "    print(f\"  - Some rare tokens (bottom 5):\")\n",
    "    for i in range(max(0, len(sorted_tokens) - 5), len(sorted_tokens)):\n",
    "        token = sorted_tokens[i].item()\n",
    "        count = sorted_counts[i].item()\n",
    "        percentage = (count / total_tokens) * 100\n",
    "        print(f\"    Token {token}: {count} occurrences ({percentage:.2f}%)\")\n",
    "    \n",
    "    return vocab_size, {\n",
    "        'unique_tokens': unique_tokens,\n",
    "        'counts': counts,\n",
    "        'min_token': min_token,\n",
    "        'max_token': max_token,\n",
    "        'total_tokens': total_tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3cbcb5-f4e3-4bb8-aec0-b8ecf6b7f2b2",
   "metadata": {},
   "source": [
    "## Create token mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "875ab8b8-8e37-4b0e-9c4c-8409da187d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_mapping(unique_tokens):\n",
    "    \"\"\"\n",
    "    Creates a mapping from original tokens to contiguous indices.\n",
    "    This is important for efficiency - we want our embedding layer to have\n",
    "    size equal to vocab_size, not max_token_value.\n",
    "    \"\"\"\n",
    "    print(\"Creating token mapping for efficient embedding...\")\n",
    "    \n",
    "    # Create mapping from original token to index\n",
    "    token_to_idx = {token.item(): idx for idx, token in enumerate(unique_tokens)}\n",
    "    idx_to_token = {idx: token.item() for idx, token in enumerate(unique_tokens)}\n",
    "    \n",
    "    print(f\"  - Mapped {len(unique_tokens)} tokens to indices 0-{len(unique_tokens)-1}\")\n",
    "    \n",
    "    return token_to_idx, idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0a0e597-9159-4c2e-813d-cfbfa9a63e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_dataset(data, token_to_idx):\n",
    "    \"\"\"\n",
    "    Remaps the dataset to use contiguous indices instead of original token values.\n",
    "    \"\"\"\n",
    "    print(\"Remapping dataset to use contiguous indices...\")\n",
    "    \n",
    "    # Create a new tensor with remapped values\n",
    "    remapped_data = torch.zeros_like(data)\n",
    "    for i, token in enumerate(data):\n",
    "        remapped_data[i] = token_to_idx[token.item()]\n",
    "    \n",
    "    print(f\"  - Remapped {len(data)} tokens\")\n",
    "    print(f\"  - Original token range: {data.min().item()} to {data.max().item()}\")\n",
    "    print(f\"  - Remapped token range: {remapped_data.min().item()} to {remapped_data.max().item()}\")\n",
    "    \n",
    "    return remapped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d367d23-d970-4044-a6c8-cb1359fc298e",
   "metadata": {},
   "source": [
    "## Creating raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22d59dd3-4eb4-4249-b4fa-7ada3e3f7596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random dataset of 100000 tokens with max value 100...\n",
      "Sample of raw data: tensor([23,  4,  4, 57, 57, 37, 48,  5,  3, 24, 42,  3, 17, 19,  4, 34, 38, 12,\n",
      "         1, 44])...\n",
      "Analyzing vocabulary from the dataset...\n",
      "Vocabulary Analysis Results:\n",
      "  - Total unique tokens (vocab size): 100\n",
      "  - Token range: 0 to 99\n",
      "  - Total tokens in dataset: 100000\n",
      "  - Average token frequency: 1000.00\n",
      "  - Top 10 most frequent tokens:\n",
      "    Token 19: 2896 occurrences (2.90%)\n",
      "    Token 7: 2884 occurrences (2.88%)\n",
      "    Token 10: 2880 occurrences (2.88%)\n",
      "    Token 0: 2867 occurrences (2.87%)\n",
      "    Token 21: 2864 occurrences (2.86%)\n",
      "    Token 8: 2850 occurrences (2.85%)\n",
      "    Token 13: 2833 occurrences (2.83%)\n",
      "    Token 1: 2826 occurrences (2.83%)\n",
      "    Token 24: 2821 occurrences (2.82%)\n",
      "    Token 17: 2819 occurrences (2.82%)\n",
      "  - Some rare tokens (bottom 5):\n",
      "    Token 58: 50 occurrences (0.05%)\n",
      "    Token 79: 45 occurrences (0.04%)\n",
      "    Token 97: 45 occurrences (0.04%)\n",
      "    Token 83: 41 occurrences (0.04%)\n",
      "    Token 84: 38 occurrences (0.04%)\n",
      "Creating token mapping for efficient embedding...\n",
      "  - Mapped 100 tokens to indices 0-99\n",
      "Remapping dataset to use contiguous indices...\n",
      "  - Remapped 100000 tokens\n",
      "  - Original token range: 0 to 99\n",
      "  - Remapped token range: 0 to 99\n",
      "Sample of remapped data: tensor([23,  4,  4, 57, 57, 37, 48,  5,  3, 24, 42,  3, 17, 19,  4, 34, 38, 12,\n",
      "         1, 44])...\n",
      "\n",
      "--- DYNAMIC VOCABULARY SIZE: 100 ---\n",
      "This will be used for embedding and output layer dimensions.\n"
     ]
    }
   ],
   "source": [
    "# Generate the raw dataset\n",
    "print(f\"Generating random dataset of {DATASET_SIZE} tokens with max value {MAX_TOKEN_VALUE}...\")\n",
    "raw_train_data = generate_random_sequence(DATASET_SIZE, MAX_TOKEN_VALUE)\n",
    "print(f\"Sample of raw data: {raw_train_data[:20]}...\")\n",
    "\n",
    "# Analyze vocabulary and get dynamic vocab size\n",
    "VOCAB_SIZE, vocab_stats = analyze_vocabulary(raw_train_data)\n",
    "\n",
    "# Create token mapping for efficient embedding\n",
    "token_to_idx, idx_to_token = create_token_mapping(vocab_stats['unique_tokens'])\n",
    "\n",
    "# Remap the dataset to use contiguous indices\n",
    "train_data = remap_dataset(raw_train_data, token_to_idx)\n",
    "print(f\"Sample of remapped data: {train_data[:20]}...\")\n",
    "\n",
    "# Move the dataset to the selected device\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "print(f\"\\n--- DYNAMIC VOCABULARY SIZE: {VOCAB_SIZE} ---\")\n",
    "print(f\"This will be used for embedding and output layer dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0edbf-bf7a-47d6-a0e0-f7b17b3fdeda",
   "metadata": {},
   "source": [
    "## Function to get batch of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85ee270d-39ee-4325-9cb0-b34f1ea48840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's see an example of a single batch with batch_size=1 and seq_length=5:\n",
      "Source (x): [6, 10, 40, 4, 23]\n",
      "Target (y): [10, 40, 4, 23, 19]\n",
      "Notice that the target is the source sequence shifted one position to the right.\n"
     ]
    }
   ],
   "source": [
    "def get_batch(source_data, seq_length, batch_size):\n",
    "    \"\"\"\n",
    "    Generates a batch of source and target sequences for training.\n",
    "    This is the core of how we set up the \"next token prediction\" task.\n",
    "    \"\"\"\n",
    "    # Get the total length of the dataset\n",
    "    num_tokens = len(source_data)\n",
    "    # Generate random starting points for our sequences within the dataset\n",
    "    # We subtract seq_length + 1 to ensure we have a valid target for each sequence\n",
    "    start_indices = torch.randint(0, num_tokens - seq_length - 1, (batch_size,))\n",
    "\n",
    "    # Create the source sequences (input to the model)\n",
    "    # torch.stack builds a new tensor from a list of tensors\n",
    "    x = torch.stack([source_data[i : i + seq_length] for i in start_indices])\n",
    "\n",
    "    # Create the target sequences (what the model should predict)\n",
    "    # The target for each token in the input is the very next token in the sequence.\n",
    "    y = torch.stack([source_data[i + 1 : i + seq_length + 1] for i in start_indices])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "print(f\"\\nLet's see an example of a single batch with batch_size=1 and seq_length=5:\")\n",
    "x_sample, y_sample = get_batch(train_data, 5, 1)\n",
    "print(f\"Source (x): {x_sample.squeeze().tolist()}\")\n",
    "print(f\"Target (y): {y_sample.squeeze().tolist()}\")\n",
    "print(\"Notice that the target is the source sequence shifted one position to the right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef213a-8c60-4bdd-85fc-4a33ca50ce71",
   "metadata": {},
   "source": [
    "## Create positional encoding since transformers see all permutations as same. Need this to make sure sequence is learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f3e131d-5c52-4ca7-9c07-803d6f6ad469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects position information into the token embeddings.\n",
    "    Since the Transformer architecture itself doesn't have a notion of order,\n",
    "    we add these positional encodings to the input embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a matrix for positional encodings of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # Create a position tensor [0, 1, 2, ..., max_len-1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Calculate the division term for the sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices in the array; 2i\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices in the array; 2i+1\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension to the positional encoding matrix\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # Register 'pe' as a buffer. Buffers are part of the model's state,\n",
    "        # but they are not considered model parameters to be trained.\n",
    "        self.register_buffer('pe', pe)\n",
    "        print(\"Initialized PositionalEncoding module.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Add the positional encoding to the input tensor\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5028858-2500-4d50-8692-9d5069b5c617",
   "metadata": {},
   "source": [
    "## The transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff5bc911-c049-46bc-b545-881d5146b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer model for sequence-to-sequence tasks.\n",
    "    In our case, it's used for next-token prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, nhead, d_hid, nlayers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # 1. Token Embedding Layer: Maps input token indices to dense vectors.\n",
    "        self.encoder = nn.Embedding(vocab_size, d_model)\n",
    "        print(f\"Initialized nn.Embedding: maps {vocab_size} tokens to {d_model}-dim vectors.\")\n",
    "\n",
    "        # 2. Positional Encoding: Adds positional information.\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # 3. Transformer Encoder Layers: The core of the model.\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        print(f\"Initialized nn.TransformerEncoder with {nlayers} layers.\")\n",
    "\n",
    "        # 4. Final Linear Layer (Decoder): Maps the Transformer output back to the vocabulary space.\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "        print(f\"Initialized final nn.Linear decoder: maps {d_model}-dim vectors to {vocab_size} (vocab size) logits.\")\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initializes weights for the embedding and linear layers.\"\"\"\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required). Shape: [seq_len, batch_size].\n",
    "            src_mask: the mask for the src sequence (required).\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Inside Model Forward Pass ---\")\n",
    "        print(f\"Input `src` shape: {src.shape} [Sequence Length, Batch Size]\")\n",
    "\n",
    "        # 1. Embed the tokens and scale by sqrt(d_model)\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        print(f\"Shape after Embedding and Scaling: {src.shape} [Seq Len, Batch, Embedding Dim]\")\n",
    "\n",
    "        # 2. Add positional encoding\n",
    "        src = self.pos_encoder(src)\n",
    "        print(f\"Shape after Positional Encoding: {src.shape} [Seq Len, Batch, Embedding Dim]\")\n",
    "\n",
    "        # 3. Pass through the Transformer encoder layers\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        print(f\"Shape after Transformer Encoder: {output.shape} [Seq Len, Batch, Embedding Dim]\")\n",
    "\n",
    "        # 4. Decode the output to get logits for each token in the vocabulary\n",
    "        output = self.decoder(output)\n",
    "        print(f\"Shape after Final Decoder Layer: {output.shape} [Seq Len, Batch, Vocab Size]\")\n",
    "        print(\"--- End of Model Forward Pass ---\\n\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062bd820-7949-49b6-98ab-1c6a473f4235",
   "metadata": {},
   "source": [
    "## Creating mask to prevent the model from seeing future tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e6cff0f-3fd8-4f2a-8302-5aa7a67dab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"\n",
    "    Generates a square causal mask for the sequence.\n",
    "    The masked positions are filled with -inf.\n",
    "    Unmasked positions are 0. This prevents the model from \"cheating\" by\n",
    "    looking at future tokens during training.\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a2c02-c192-4a79-aee1-ca8f21d096d0",
   "metadata": {},
   "source": [
    "## Initialize Model, loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee020b2b-8b0f-46ec-8777-c28ae2db3fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized nn.Embedding: maps 100 tokens to 256-dim vectors.\n",
      "Initialized PositionalEncoding module.\n",
      "Initialized nn.TransformerEncoder with 4 layers.\n",
      "Initialized final nn.Linear decoder: maps 256-dim vectors to 100 (vocab size) logits.\n",
      "Model initialized with dynamic vocabulary size: 100\n",
      "Total model parameters: 3,210,340\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with the dynamically calculated vocab size\n",
    "model = TransformerModel(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, FF_DIM, NUM_ENCODER_LAYERS, DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Model initialized with dynamic vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"Total model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c6a44-ecb4-45a6-8593-77c32f40c1db",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fa4ccd0-0a4d-4859-8713-6fa7b0507d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Epoch 1 ---\n",
      "Shape of data batch fed to model: torch.Size([64, 32])\n",
      "Shape of target batch for loss: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "\n",
      "--- Inside Model Forward Pass ---\n",
      "Input `src` shape: torch.Size([64, 32]) [Sequence Length, Batch Size]\n",
      "Shape after Embedding and Scaling: torch.Size([64, 32, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Positional Encoding: torch.Size([64, 32, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Transformer Encoder: torch.Size([64, 32, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Final Decoder Layer: torch.Size([64, 32, 100]) [Seq Len, Batch, Vocab Size]\n",
      "--- End of Model Forward Pass ---\n",
      "\n",
      "| epoch   1 |   200/ 1536 batches | lr 0.00100 | ms/batch 14.24 | loss  4.00 | ppl    54.75\n",
      "| epoch   1 |   400/ 1536 batches | lr 0.00100 | ms/batch 12.95 | loss  3.96 | ppl    52.43\n",
      "| epoch   1 |   600/ 1536 batches | lr 0.00100 | ms/batch 12.93 | loss  3.95 | ppl    51.85\n",
      "| epoch   1 |   800/ 1536 batches | lr 0.00100 | ms/batch 12.91 | loss  3.94 | ppl    51.55\n",
      "| epoch   1 |  1000/ 1536 batches | lr 0.00100 | ms/batch 13.03 | loss  3.94 | ppl    51.33\n",
      "| epoch   1 |  1200/ 1536 batches | lr 0.00100 | ms/batch 13.09 | loss  3.94 | ppl    51.31\n",
      "| epoch   1 |  1400/ 1536 batches | lr 0.00100 | ms/batch 13.00 | loss  3.93 | ppl    51.01\n",
      "--- End of Epoch 1 | Time: 20.54s ---\n",
      "\n",
      "--- Starting Epoch 2 ---\n",
      "Shape of data batch fed to model: torch.Size([64, 32])\n",
      "Shape of target batch for loss: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "| epoch   2 |   200/ 1536 batches | lr 0.00100 | ms/batch 13.09 | loss  3.95 | ppl    51.79\n",
      "| epoch   2 |   400/ 1536 batches | lr 0.00100 | ms/batch 13.00 | loss  3.93 | ppl    50.72\n",
      "| epoch   2 |   600/ 1536 batches | lr 0.00100 | ms/batch 13.01 | loss  3.92 | ppl    50.57\n",
      "| epoch   2 |   800/ 1536 batches | lr 0.00100 | ms/batch 13.04 | loss  3.92 | ppl    50.36\n",
      "| epoch   2 |  1000/ 1536 batches | lr 0.00100 | ms/batch 13.05 | loss  3.91 | ppl    50.07\n",
      "| epoch   2 |  1200/ 1536 batches | lr 0.00100 | ms/batch 13.05 | loss  3.91 | ppl    49.94\n",
      "| epoch   2 |  1400/ 1536 batches | lr 0.00100 | ms/batch 13.04 | loss  3.90 | ppl    49.46\n",
      "--- End of Epoch 2 | Time: 20.38s ---\n",
      "\n",
      "--- Starting Epoch 3 ---\n",
      "Shape of data batch fed to model: torch.Size([64, 32])\n",
      "Shape of target batch for loss: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "| epoch   3 |   200/ 1536 batches | lr 0.00100 | ms/batch 13.15 | loss  3.91 | ppl    49.86\n",
      "| epoch   3 |   400/ 1536 batches | lr 0.00100 | ms/batch 13.08 | loss  3.87 | ppl    48.12\n",
      "| epoch   3 |   600/ 1536 batches | lr 0.00100 | ms/batch 13.09 | loss  3.86 | ppl    47.58\n",
      "| epoch   3 |   800/ 1536 batches | lr 0.00100 | ms/batch 13.09 | loss  3.85 | ppl    46.77\n",
      "| epoch   3 |  1000/ 1536 batches | lr 0.00100 | ms/batch 13.12 | loss  3.83 | ppl    45.95\n",
      "| epoch   3 |  1200/ 1536 batches | lr 0.00100 | ms/batch 13.12 | loss  3.81 | ppl    45.01\n",
      "| epoch   3 |  1400/ 1536 batches | lr 0.00100 | ms/batch 13.15 | loss  3.79 | ppl    44.15\n",
      "--- End of Epoch 3 | Time: 20.48s ---\n",
      "\n",
      "--- Starting Epoch 4 ---\n",
      "Shape of data batch fed to model: torch.Size([64, 32])\n",
      "Shape of target batch for loss: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "| epoch   4 |   200/ 1536 batches | lr 0.00100 | ms/batch 13.22 | loss  3.76 | ppl    42.76\n",
      "| epoch   4 |   400/ 1536 batches | lr 0.00100 | ms/batch 13.19 | loss  3.71 | ppl    40.80\n",
      "| epoch   4 |   600/ 1536 batches | lr 0.00100 | ms/batch 13.17 | loss  3.68 | ppl    39.72\n",
      "| epoch   4 |   800/ 1536 batches | lr 0.00100 | ms/batch 13.19 | loss  3.65 | ppl    38.60\n",
      "| epoch   4 |  1000/ 1536 batches | lr 0.00100 | ms/batch 13.20 | loss  3.62 | ppl    37.51\n",
      "| epoch   4 |  1200/ 1536 batches | lr 0.00100 | ms/batch 13.18 | loss  3.60 | ppl    36.54\n",
      "| epoch   4 |  1400/ 1536 batches | lr 0.00100 | ms/batch 13.21 | loss  3.58 | ppl    35.71\n",
      "--- End of Epoch 4 | Time: 20.60s ---\n",
      "\n",
      "--- Starting Epoch 5 ---\n",
      "Shape of data batch fed to model: torch.Size([64, 32])\n",
      "Shape of target batch for loss: torch.Size([64, 32])\n",
      "Shape of causal mask: torch.Size([64, 64])\n",
      "Starting batch iterations...\n",
      "| epoch   5 |   200/ 1536 batches | lr 0.00100 | ms/batch 13.30 | loss  3.54 | ppl    34.53\n",
      "| epoch   5 |   400/ 1536 batches | lr 0.00100 | ms/batch 13.22 | loss  3.49 | ppl    32.94\n",
      "| epoch   5 |   600/ 1536 batches | lr 0.00100 | ms/batch 13.22 | loss  3.47 | ppl    32.06\n",
      "| epoch   5 |   800/ 1536 batches | lr 0.00100 | ms/batch 13.24 | loss  3.45 | ppl    31.41\n",
      "| epoch   5 |  1000/ 1536 batches | lr 0.00100 | ms/batch 13.24 | loss  3.42 | ppl    30.46\n",
      "| epoch   5 |  1200/ 1536 batches | lr 0.00100 | ms/batch 13.26 | loss  3.40 | ppl    29.86\n",
      "| epoch   5 |  1400/ 1536 batches | lr 0.00100 | ms/batch 13.27 | loss  3.37 | ppl    29.20\n",
      "--- End of Epoch 5 | Time: 20.69s ---\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    \"\"\"Defines the training loop for one epoch.\"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    # Generate the causal mask. It's the same for all sequences of the same length.\n",
    "    src_mask = generate_square_subsequent_mask(SEQUENCE_LENGTH).to(device)\n",
    "\n",
    "    # Calculate the number of batches in one epoch\n",
    "    num_batches = len(train_data) // (SEQUENCE_LENGTH * BATCH_SIZE)\n",
    "\n",
    "    print(f\"\\n--- Starting Epoch {epoch} ---\")\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1 - SEQUENCE_LENGTH, SEQUENCE_LENGTH)):\n",
    "        # Get a batch of data\n",
    "        data, targets = get_batch(train_data, SEQUENCE_LENGTH, BATCH_SIZE)\n",
    "        \n",
    "        # The model expects inputs of shape [sequence_length, batch_size]\n",
    "        # Our get_batch function returns [batch_size, sequence_length], so we permute it.\n",
    "        data = data.permute(1, 0)\n",
    "        targets = targets.permute(1, 0)\n",
    "        \n",
    "        # The first time through, print shapes to be extra clear\n",
    "        if batch == 0:\n",
    "            print(f\"Shape of data batch fed to model: {data.shape}\")\n",
    "            print(f\"Shape of target batch for loss: {targets.shape}\")\n",
    "            print(f\"Shape of causal mask: {src_mask.shape}\")\n",
    "            print(\"Starting batch iterations...\")\n",
    "\n",
    "        optimizer.zero_grad() # Reset gradients\n",
    "\n",
    "        # This is where we stop printing the forward pass details to avoid clutter\n",
    "        # We'll only do it once during the prediction phase.\n",
    "        if batch == 0 and epoch == 1:\n",
    "            output = model(data, src_mask) # Run the forward pass\n",
    "        else:\n",
    "            # Temporarily disable the print statements in the forward pass\n",
    "            # for cleaner training logs.\n",
    "            _print = __builtins__.print\n",
    "            __builtins__.print = lambda *args, **kwargs: None\n",
    "            output = model(data, src_mask)\n",
    "            __builtins__.print = _print\n",
    "\n",
    "        # Reshape the output and targets for the loss function\n",
    "        # The loss function expects [Batch * SeqLen, VocabSize] and [Batch * SeqLen]\n",
    "        loss = criterion(output.view(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "\n",
    "        loss.backward() # Compute gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # Prevent exploding gradients\n",
    "        optimizer.step() # Update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Log progress\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / LOG_INTERVAL\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches * BATCH_SIZE:5d} batches | '\n",
    "                  f'lr {lr:02.5f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(epoch)\n",
    "    print(f'--- End of Epoch {epoch} | Time: {(time.time() - epoch_start_time):.2f}s ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674eeccc-a469-4c0a-9b98-5f6e5c64f94a",
   "metadata": {},
   "source": [
    "## Generating new sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0903e463-cc06-4f5d-9a80-169e5d157ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, seed_sequence, max_len=50, idx_to_token=None):\n",
    "    \"\"\"\n",
    "    Generates a sequence token by token based on a seed.\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    print(f\"Seed sequence (indices): {seed_sequence}\")\n",
    "    if idx_to_token:\n",
    "        original_tokens = [idx_to_token[idx] for idx in seed_sequence]\n",
    "        print(f\"Seed sequence (original tokens): {original_tokens}\")\n",
    "    \n",
    "    # Convert the seed sequence (list of ints) to a tensor\n",
    "    input_tensor = torch.tensor(seed_sequence, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    \n",
    "    generated_sequence = seed_sequence.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_len):\n",
    "            print(f\"\\n--- Prediction step {step + 1} ---\")\n",
    "            \n",
    "            # Create the causal mask for the current sequence length\n",
    "            current_seq_len = input_tensor.size(0)\n",
    "            mask = generate_square_subsequent_mask(current_seq_len).to(device)\n",
    "            \n",
    "            # Get the model's output. We will re-enable printing for this one pass.\n",
    "            if step == 0:  # Only print details for the first step\n",
    "                output = model(input_tensor, mask)\n",
    "            else:\n",
    "                # Disable printing for subsequent steps\n",
    "                _print = __builtins__.print\n",
    "                __builtins__.print = lambda *args, **kwargs: None\n",
    "                output = model(input_tensor, mask)\n",
    "                __builtins__.print = _print\n",
    "            \n",
    "            # We only care about the prediction for the VERY LAST token in the input sequence.\n",
    "            # The output shape is [seq_len, batch_size, vocab_size].\n",
    "            # We take the last token's output: output[-1, 0, :]\n",
    "            last_token_logits = output[-1, 0, :]\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "            \n",
    "            # Choose the next token. We can use argmax for the most likely token,\n",
    "            # or sample from the distribution for more variety.\n",
    "            # next_token = torch.argmax(probabilities).item()\n",
    "            next_token = torch.multinomial(probabilities, 1).item()\n",
    "            \n",
    "            predicted_original = idx_to_token[next_token] if idx_to_token else next_token\n",
    "            print(f\"Model predicted next token: {next_token} (original: {predicted_original})\")\n",
    "            \n",
    "            # Append the predicted token to our sequence\n",
    "            generated_sequence.append(next_token)\n",
    "            \n",
    "            # Create the new input for the next iteration by appending the predicted token\n",
    "            input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=device)], dim=0)\n",
    "\n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014cc7a-99f7-42eb-9351-2da8a4787f31",
   "metadata": {},
   "source": [
    "## Generating new sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0067d05-36c2-40cf-8636-b8e8c411a170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed sequence (indices): [0, 1, 2, 3, 4]\n",
      "Seed sequence (original tokens): [0, 1, 2, 3, 4]\n",
      "\n",
      "--- Prediction step 1 ---\n",
      "\n",
      "--- Inside Model Forward Pass ---\n",
      "Input `src` shape: torch.Size([5, 1]) [Sequence Length, Batch Size]\n",
      "Shape after Embedding and Scaling: torch.Size([5, 1, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Positional Encoding: torch.Size([5, 1, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Transformer Encoder: torch.Size([5, 1, 256]) [Seq Len, Batch, Embedding Dim]\n",
      "Shape after Final Decoder Layer: torch.Size([5, 1, 100]) [Seq Len, Batch, Vocab Size]\n",
      "--- End of Model Forward Pass ---\n",
      "\n",
      "Model predicted next token: 18 (original: 18)\n",
      "\n",
      "--- Prediction step 2 ---\n",
      "Model predicted next token: 7 (original: 7)\n",
      "\n",
      "--- Prediction step 3 ---\n",
      "Model predicted next token: 3 (original: 3)\n",
      "\n",
      "--- Prediction step 4 ---\n",
      "Model predicted next token: 30 (original: 30)\n",
      "\n",
      "--- Prediction step 5 ---\n",
      "Model predicted next token: 14 (original: 14)\n",
      "\n",
      "--- Prediction step 6 ---\n",
      "Model predicted next token: 6 (original: 6)\n",
      "\n",
      "--- Prediction step 7 ---\n",
      "Model predicted next token: 5 (original: 5)\n",
      "\n",
      "--- Prediction step 8 ---\n",
      "Model predicted next token: 39 (original: 39)\n",
      "\n",
      "--- Prediction step 9 ---\n",
      "Model predicted next token: 3 (original: 3)\n",
      "\n",
      "--- Prediction step 10 ---\n",
      "Model predicted next token: 24 (original: 24)\n",
      "\n",
      "--- Prediction step 11 ---\n",
      "Model predicted next token: 36 (original: 36)\n",
      "\n",
      "--- Prediction step 12 ---\n",
      "Model predicted next token: 24 (original: 24)\n",
      "\n",
      "--- Prediction step 13 ---\n",
      "Model predicted next token: 24 (original: 24)\n",
      "\n",
      "--- Prediction step 14 ---\n",
      "Model predicted next token: 44 (original: 44)\n",
      "\n",
      "--- Prediction step 15 ---\n",
      "Model predicted next token: 33 (original: 33)\n",
      "\n",
      "--- Prediction step 16 ---\n",
      "Model predicted next token: 15 (original: 15)\n",
      "\n",
      "--- Prediction step 17 ---\n",
      "Model predicted next token: 42 (original: 42)\n",
      "\n",
      "--- Prediction step 18 ---\n",
      "Model predicted next token: 24 (original: 24)\n",
      "\n",
      "--- Prediction step 19 ---\n",
      "Model predicted next token: 10 (original: 10)\n",
      "\n",
      "--- Prediction step 20 ---\n",
      "Model predicted next token: 1 (original: 1)\n",
      "\n",
      "\n",
      "--- Final Result ---\n",
      "Original Seed (indices): [0, 1, 2, 3, 4]\n",
      "Original Seed (original tokens): [0, 1, 2, 3, 4]\n",
      "Generated Sequence (indices): [0, 1, 2, 3, 4, 18, 7, 3, 30, 14, 6, 5, 39, 3, 24, 36, 24, 24, 44, 33, 15, 42, 24, 10, 1]\n",
      "Generated Sequence (original tokens): [0, 1, 2, 3, 4, 18, 7, 3, 30, 14, 6, 5, 39, 3, 24, 36, 24, 24, 44, 33, 15, 42, 24, 10, 1]\n",
      "\n",
      "Training and prediction complete!\n",
      "Final vocabulary size used: 100\n"
     ]
    }
   ],
   "source": [
    "# Let's generate a new sequence using some tokens from our vocabulary\n",
    "# We'll use the first few tokens from our remapped vocabulary\n",
    "seed_indices = [0, 1, 2, 3, 4]  # These are indices in our remapped space\n",
    "predicted_sequence = predict(model, seed_indices, max_len=20, idx_to_token=idx_to_token)\n",
    "\n",
    "print(\"\\n\\n--- Final Result ---\")\n",
    "print(f\"Original Seed (indices): {seed_indices}\")\n",
    "print(f\"Original Seed (original tokens): {[idx_to_token[idx] for idx in seed_indices]}\")\n",
    "print(f\"Generated Sequence (indices): {predicted_sequence}\")\n",
    "print(f\"Generated Sequence (original tokens): {[idx_to_token[idx] for idx in predicted_sequence]}\")\n",
    "print(\"\\nTraining and prediction complete!\")\n",
    "print(f\"Final vocabulary size used: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162416b-e7c4-4765-9a5a-0011ead9dd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
